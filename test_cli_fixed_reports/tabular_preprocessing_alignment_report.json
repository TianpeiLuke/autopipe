{
  "script_name": "tabular_preprocessing",
  "level1": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "arguments",
        "message": "Script defines config-driven argument provided by builder: --job-type (accessed as args.job_type)",
        "details": {
          "cli_argument": "job-type",
          "python_attribute": "job_type",
          "script": "tabular_preprocessing",
          "source": "builder"
        },
        "recommendation": "Argument --job-type is provided by builder - no action needed"
      }
    ],
    "script_analysis": {
      "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/tabular_preprocessing.py",
      "path_references": [
        "path='.gz' line_number=18 context='\\ndef _is_gzipped(path: str) -> bool:\\n>>>     return path.lower().endswith(\".gz\")\\n\\ndef _detect_separator_from_sample(sample_lines: str) -> str:' is_hardcoded=True construction_method=None",
        "path='Use csv.Sniffer to detect a delimiter, defaulting to comma.' line_number=21 context='\\ndef _detect_separator_from_sample(sample_lines: str) -> str:\\n>>>     \"\"\"Use csv.Sniffer to detect a delimiter, defaulting to comma.\"\"\"\\n    try:\\n        dialect = csv.Sniffer().sniff(sample_lines)' is_hardcoded=True construction_method=None",
        "path='Check if the JSON file is in JSON Lines or regular format.' line_number=29 context='\\ndef peek_json_format(file_path: Path, open_func=open) -> str:\\n>>>     \"\"\"Check if the JSON file is in JSON Lines or regular format.\"\"\"\\n    try:\\n        with open_func(str(file_path), \"rt\") as f:' is_hardcoded=True construction_method=None",
        "path='Read a JSON or JSON Lines file into a DataFrame.' line_number=48 context='\\ndef _read_json_file(file_path: Path) -> pd.DataFrame:\\n>>>     \"\"\"Read a JSON or JSON Lines file into a DataFrame.\"\"\"\\n    open_func = gzip.open if _is_gzipped(str(file_path)) else open\\n    fmt = peek_json_format(file_path, open_func)' is_hardcoded=True construction_method=None",
        "path='Read a single file (CSV, TSV, JSON, Parquet) into a DataFrame.' line_number=59 context='\\ndef _read_file_to_df(file_path: Path) -> pd.DataFrame:\\n>>>     \"\"\"Read a single file (CSV, TSV, JSON, Parquet) into a DataFrame.\"\"\"\\n    suffix = file_path.suffix.lower()\\n    if suffix == \".gz\":' is_hardcoded=True construction_method=None",
        "path='.gz' line_number=61 context='    \"\"\"Read a single file (CSV, TSV, JSON, Parquet) into a DataFrame.\"\"\"\\n    suffix = file_path.suffix.lower()\\n>>>     if suffix == \".gz\":\\n        inner_ext = Path(file_path.stem).suffix.lower()\\n        if inner_ext in [\".csv\", \".tsv\"]:' is_hardcoded=True construction_method=None",
        "path='.csv' line_number=63 context='    if suffix == \".gz\":\\n        inner_ext = Path(file_path.stem).suffix.lower()\\n>>>         if inner_ext in [\".csv\", \".tsv\"]:\\n            with gzip.open(str(file_path), \"rt\") as f:\\n                sep = _detect_separator_from_sample(f.readline() + f.readline())' is_hardcoded=True construction_method=None",
        "path='.tsv' line_number=63 context='    if suffix == \".gz\":\\n        inner_ext = Path(file_path.stem).suffix.lower()\\n>>>         if inner_ext in [\".csv\", \".tsv\"]:\\n            with gzip.open(str(file_path), \"rt\") as f:\\n                sep = _detect_separator_from_sample(f.readline() + f.readline())' is_hardcoded=True construction_method=None",
        "path='.json' line_number=67 context='                sep = _detect_separator_from_sample(f.readline() + f.readline())\\n            return pd.read_csv(str(file_path), sep=sep, compression=\"gzip\")\\n>>>         elif inner_ext == \".json\":\\n            return _read_json_file(file_path)\\n        elif inner_ext.endswith(\".parquet\"):' is_hardcoded=True construction_method=None",
        "path='.csv' line_number=78 context='        else:\\n            raise ValueError(f\"Unsupported gzipped file type: {file_path}\")\\n>>>     elif suffix in [\".csv\", \".tsv\"]:\\n        with open(str(file_path), \"rt\") as f:\\n            sep = _detect_separator_from_sample(f.readline() + f.readline())' is_hardcoded=True construction_method=None",
        "path='.tsv' line_number=78 context='        else:\\n            raise ValueError(f\"Unsupported gzipped file type: {file_path}\")\\n>>>     elif suffix in [\".csv\", \".tsv\"]:\\n        with open(str(file_path), \"rt\") as f:\\n            sep = _detect_separator_from_sample(f.readline() + f.readline())' is_hardcoded=True construction_method=None",
        "path='.json' line_number=82 context='            sep = _detect_separator_from_sample(f.readline() + f.readline())\\n        return pd.read_csv(str(file_path), sep=sep)\\n>>>     elif suffix == \".json\":\\n        return _read_json_file(file_path)\\n    elif suffix.endswith(\".parquet\"):' is_hardcoded=True construction_method=None",
        "path='Detect and combine all supported data shards in a directory.' line_number=90 context='\\ndef combine_shards(input_dir: str) -> pd.DataFrame:\\n>>>     \"\"\"Detect and combine all supported data shards in a directory.\"\"\"\\n    input_path = Path(input_dir)\\n    if not input_path.is_dir():' is_hardcoded=True construction_method=None",
        "path='part-*.csv' line_number=95 context='        raise RuntimeError(f\"Input directory does not exist: {input_dir}\")\\n    patterns = [\\n>>>         \"part-*.csv\", \"part-*.csv.gz\", \"part-*.json\", \"part-*.json.gz\",\\n        \"part-*.parquet\", \"part-*.snappy.parquet\", \"part-*.parquet.gz\"\\n    ]' is_hardcoded=True construction_method=None",
        "path='part-*.csv.gz' line_number=95 context='        raise RuntimeError(f\"Input directory does not exist: {input_dir}\")\\n    patterns = [\\n>>>         \"part-*.csv\", \"part-*.csv.gz\", \"part-*.json\", \"part-*.json.gz\",\\n        \"part-*.parquet\", \"part-*.snappy.parquet\", \"part-*.parquet.gz\"\\n    ]' is_hardcoded=True construction_method=None",
        "path='part-*.json' line_number=95 context='        raise RuntimeError(f\"Input directory does not exist: {input_dir}\")\\n    patterns = [\\n>>>         \"part-*.csv\", \"part-*.csv.gz\", \"part-*.json\", \"part-*.json.gz\",\\n        \"part-*.parquet\", \"part-*.snappy.parquet\", \"part-*.parquet.gz\"\\n    ]' is_hardcoded=True construction_method=None",
        "path='part-*.json.gz' line_number=95 context='        raise RuntimeError(f\"Input directory does not exist: {input_dir}\")\\n    patterns = [\\n>>>         \"part-*.csv\", \"part-*.csv.gz\", \"part-*.json\", \"part-*.json.gz\",\\n        \"part-*.parquet\", \"part-*.snappy.parquet\", \"part-*.parquet.gz\"\\n    ]' is_hardcoded=True construction_method=None",
        "path='part-*.parquet.gz' line_number=96 context='    patterns = [\\n        \"part-*.csv\", \"part-*.csv.gz\", \"part-*.json\", \"part-*.json.gz\",\\n>>>         \"part-*.parquet\", \"part-*.snappy.parquet\", \"part-*.parquet.gz\"\\n    ]\\n    all_shards = sorted([p for pat in patterns for p in input_path.glob(pat)])' is_hardcoded=True construction_method=None",
        "path='No CSV/JSON/Parquet shards found under ' line_number=100 context='    all_shards = sorted([p for pat in patterns for p in input_path.glob(pat)])\\n    if not all_shards:\\n>>>         raise RuntimeError(f\"No CSV/JSON/Parquet shards found under {input_dir}\")\\n    try:\\n        dfs = [_read_file_to_df(shard) for shard in all_shards]' is_hardcoded=True construction_method=None",
        "path='\\n    Main logic for preprocessing data, now refactored for testability.\\n    ' line_number=110 context='\\ndef main(job_type: str, label_field: str, train_ratio: float, test_val_ratio: float, input_data_dir: str, output_dir: str):\\n>>>     \"\"\"\\n    Main logic for preprocessing data, now refactored for testability.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='.' line_number=123 context='\\n    # 3. Process columns and labels\\n>>>     df.columns = [col.replace(\"__DOT__\", \".\") for col in df.columns]\\n    if label_field not in df.columns:\\n        raise RuntimeError(f\"Label field \\'{label_field}\\' not found in columns: {df.columns.tolist()}\")' is_hardcoded=True construction_method=None",
        "path='_processed_data.csv' line_number=151 context='        \\n        # Only output processed_data.csv\\n>>>         proc_path = subfolder / f\"{split_name}_processed_data.csv\"\\n        split_df.to_csv(proc_path, index=False)\\n        print(f\"[INFO] Saved {proc_path} (shape={split_df.shape})\")' is_hardcoded=True construction_method=None",
        "path='[INFO] Preprocessing complete.' line_number=155 context='        print(f\"[INFO] Saved {proc_path} (shape={split_df.shape})\")\\n\\n>>>     print(\"[INFO] Preprocessing complete.\")\\n\\n' is_hardcoded=True construction_method=None",
        "path='LABEL_FIELD environment variable must be set.' line_number=166 context='    LABEL_FIELD = os.environ.get(\"LABEL_FIELD\")\\n    if not LABEL_FIELD:\\n>>>         raise RuntimeError(\"LABEL_FIELD environment variable must be set.\")\\n    TRAIN_RATIO = float(os.environ.get(\"TRAIN_RATIO\", 0.7))\\n    TEST_VAL_RATIO = float(os.environ.get(\"TEST_VAL_RATIO\", 0.5))' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/data' line_number=171 context='    \\n    # Define standard SageMaker paths - use contract-declared paths directly\\n>>>     INPUT_DATA_DIR = \"/opt/ml/processing/input/data\"  # Direct path from contract\\n    OUTPUT_DIR = \"/opt/ml/processing/output\"\\n' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output' line_number=172 context='    # Define standard SageMaker paths - use contract-declared paths directly\\n    INPUT_DATA_DIR = \"/opt/ml/processing/input/data\"  # Direct path from contract\\n>>>     OUTPUT_DIR = \"/opt/ml/processing/output\"\\n\\n    # Execute the main processing logic by calling the refactored main function' is_hardcoded=True construction_method=None"
      ],
      "env_var_accesses": [
        "variable_name='LABEL_FIELD' line_number=164 context='\\n    # Read configuration from environment variables\\n>>>     LABEL_FIELD = os.environ.get(\"LABEL_FIELD\")\\n    if not LABEL_FIELD:\\n        raise RuntimeError(\"LABEL_FIELD environment variable must be set.\")' access_method='os.environ.get' has_default=False default_value=None",
        "variable_name='TRAIN_RATIO' line_number=167 context='    if not LABEL_FIELD:\\n        raise RuntimeError(\"LABEL_FIELD environment variable must be set.\")\\n>>>     TRAIN_RATIO = float(os.environ.get(\"TRAIN_RATIO\", 0.7))\\n    TEST_VAL_RATIO = float(os.environ.get(\"TEST_VAL_RATIO\", 0.5))\\n    ' access_method='os.environ.get' has_default=True default_value=None",
        "variable_name='TEST_VAL_RATIO' line_number=168 context='        raise RuntimeError(\"LABEL_FIELD environment variable must be set.\")\\n    TRAIN_RATIO = float(os.environ.get(\"TRAIN_RATIO\", 0.7))\\n>>>     TEST_VAL_RATIO = float(os.environ.get(\"TEST_VAL_RATIO\", 0.5))\\n    \\n    # Define standard SageMaker paths - use contract-declared paths directly' access_method='os.environ.get' has_default=True default_value=None"
      ],
      "imports": [
        "module_name='os' import_alias=None line_number=2 is_from_import=False imported_items=[]",
        "module_name='gzip' import_alias=None line_number=3 is_from_import=False imported_items=[]",
        "module_name='tempfile' import_alias=None line_number=4 is_from_import=False imported_items=[]",
        "module_name='shutil' import_alias=None line_number=5 is_from_import=False imported_items=[]",
        "module_name='csv' import_alias=None line_number=6 is_from_import=False imported_items=[]",
        "module_name='json' import_alias=None line_number=7 is_from_import=False imported_items=[]",
        "module_name='argparse' import_alias=None line_number=8 is_from_import=False imported_items=[]",
        "module_name='pathlib' import_alias=None line_number=9 is_from_import=True imported_items=['Path']",
        "module_name='multiprocessing' import_alias=None line_number=10 is_from_import=True imported_items=['Pool', 'cpu_count']",
        "module_name='pandas' import_alias='pd' line_number=11 is_from_import=False imported_items=[]",
        "module_name='numpy' import_alias='np' line_number=12 is_from_import=False imported_items=[]",
        "module_name='sklearn.model_selection' import_alias=None line_number=13 is_from_import=True imported_items=['train_test_split']"
      ],
      "argument_definitions": [
        "argument_name='job_type' line_number=160 is_required=True has_default=False default_value=None argument_type='str' choices=None"
      ],
      "file_operations": [
        "file_path='<file_object>' operation_type='read' line_number=55 context='    else:\\n        with open_func(str(file_path), \"rt\") as f:\\n>>>             data = json.load(f)\\n        return pd.json_normalize(data if isinstance(data, list) else [data])\\n' mode=None method='json.load'"
      ]
    },
    "contract": {
      "entry_point": "tabular_preprocessing.py",
      "inputs": {
        "DATA": {
          "path": "/opt/ml/processing/input/data"
        }
      },
      "outputs": {
        "processed_data": {
          "path": "/opt/ml/processing/output"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [
          "LABEL_FIELD",
          "TRAIN_RATIO",
          "TEST_VAL_RATIO"
        ],
        "optional": {
          "CATEGORICAL_COLUMNS": "",
          "NUMERICAL_COLUMNS": "",
          "TEXT_COLUMNS": "",
          "DATE_COLUMNS": ""
        }
      },
      "description": "\n    Tabular preprocessing script that:\n    1. Combines data shards from input directory\n    2. Cleans and processes label field\n    3. Splits data into train/test/val for training jobs\n    4. Outputs processed CSV files by split\n    \n    Contract aligned with actual script implementation:\n    - Inputs: DATA (required) - reads from /opt/ml/processing/input/data\n    - Outputs: processed_data (primary) - writes to /opt/ml/processing/output\n    - Arguments: job_type (required) - defines processing mode (training/validation/testing)\n    \n    Script Implementation Details:\n    - Reads data shards (CSV, JSON, Parquet) from input/data directory\n    - Supports gzipped files and various formats\n    - Processes labels (converts categorical to numeric if needed)\n    - Splits data based on job_type (training creates train/test/val splits)\n    - Outputs processed files to split subdirectories under /opt/ml/processing/output\n    ",
      "framework_requirements": {
        "pandas": ">=1.3.0",
        "numpy": ">=1.21.0",
        "scikit-learn": ">=1.0.0"
      }
    }
  },
  "level2": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "multi_variant_validation",
        "message": "Smart Specification Selection: validated against 5 variants",
        "details": {
          "contract": "tabular_preprocessing",
          "variants": [
            "training",
            "testing",
            "validation",
            "calibration",
            "generic"
          ],
          "total_dependencies": 1,
          "total_outputs": 1,
          "contract_inputs": 1,
          "contract_outputs": 1
        },
        "recommendation": "Multi-variant validation completed successfully"
      }
    ],
    "contract": {
      "entry_point": "tabular_preprocessing.py",
      "inputs": {
        "DATA": {
          "path": "/opt/ml/processing/input/data"
        }
      },
      "outputs": {
        "processed_data": {
          "path": "/opt/ml/processing/output"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [
          "LABEL_FIELD",
          "TRAIN_RATIO",
          "TEST_VAL_RATIO"
        ],
        "optional": {
          "CATEGORICAL_COLUMNS": "",
          "NUMERICAL_COLUMNS": "",
          "TEXT_COLUMNS": "",
          "DATE_COLUMNS": ""
        }
      },
      "description": "\n    Tabular preprocessing script that:\n    1. Combines data shards from input directory\n    2. Cleans and processes label field\n    3. Splits data into train/test/val for training jobs\n    4. Outputs processed CSV files by split\n    \n    Contract aligned with actual script implementation:\n    - Inputs: DATA (required) - reads from /opt/ml/processing/input/data\n    - Outputs: processed_data (primary) - writes to /opt/ml/processing/output\n    - Arguments: job_type (required) - defines processing mode (training/validation/testing)\n    \n    Script Implementation Details:\n    - Reads data shards (CSV, JSON, Parquet) from input/data directory\n    - Supports gzipped files and various formats\n    - Processes labels (converts categorical to numeric if needed)\n    - Splits data based on job_type (training creates train/test/val splits)\n    - Outputs processed files to split subdirectories under /opt/ml/processing/output\n    ",
      "framework_requirements": {
        "pandas": ">=1.3.0",
        "numpy": ">=1.21.0",
        "scikit-learn": ">=1.0.0"
      }
    },
    "specifications": {
      "tabular_preprocessing_testing_spec": {
        "step_type": "TabularPreprocessing_Testing",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "DATA",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "DataLoad",
              "CradleDataLoading",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "Raw testing data for preprocessing"
          }
        ],
        "outputs": [
          {
            "logical_name": "processed_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Processed testing data"
          }
        ]
      },
      "tabular_preprocessing_calibration_spec": {
        "step_type": "TabularPreprocessing_Calibration",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "DATA",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "DataLoad",
              "CradleDataLoading",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "Raw calibration data for preprocessing"
          }
        ],
        "outputs": [
          {
            "logical_name": "processed_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Processed calibration data for model evaluation"
          }
        ]
      },
      "tabular_preprocessing_validation_spec": {
        "step_type": "TabularPreprocessing_Validation",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "DATA",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "DataLoad",
              "CradleDataLoading",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "Raw validation data for preprocessing"
          }
        ],
        "outputs": [
          {
            "logical_name": "processed_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Processed validation data"
          }
        ]
      },
      "tabular_preprocessing_training_spec": {
        "step_type": "TabularPreprocessing_Training",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "DATA",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "DataLoad",
              "CradleDataLoading",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "Raw training data for preprocessing"
          }
        ],
        "outputs": [
          {
            "logical_name": "processed_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Processed training data with train/val/test splits"
          }
        ]
      },
      "tabular_preprocessing_spec": {
        "step_type": "TabularPreprocessing_Training",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "DATA",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "DataLoad",
              "CradleDataLoading",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "Raw tabular data for preprocessing"
          }
        ],
        "outputs": [
          {
            "logical_name": "processed_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Processed tabular data with train/val/test splits"
          }
        ]
      }
    },
    "unified_specification": {
      "primary_spec": {
        "step_type": "TabularPreprocessing_Training",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "DATA",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "DataLoad",
              "CradleDataLoading",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "Raw training data for preprocessing"
          }
        ],
        "outputs": [
          {
            "logical_name": "processed_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Processed training data with train/val/test splits"
          }
        ]
      },
      "variants": {
        "training": {
          "step_type": "TabularPreprocessing_Training",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "DATA",
              "dependency_type": "processing_output",
              "required": true,
              "compatible_sources": [
                "DataLoad",
                "CradleDataLoading",
                "ProcessingStep"
              ],
              "data_type": "S3Uri",
              "description": "Raw training data for preprocessing"
            }
          ],
          "outputs": [
            {
              "logical_name": "processed_data",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Processed training data with train/val/test splits"
            }
          ]
        },
        "testing": {
          "step_type": "TabularPreprocessing_Testing",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "DATA",
              "dependency_type": "processing_output",
              "required": true,
              "compatible_sources": [
                "DataLoad",
                "CradleDataLoading",
                "ProcessingStep"
              ],
              "data_type": "S3Uri",
              "description": "Raw testing data for preprocessing"
            }
          ],
          "outputs": [
            {
              "logical_name": "processed_data",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Processed testing data"
            }
          ]
        },
        "validation": {
          "step_type": "TabularPreprocessing_Validation",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "DATA",
              "dependency_type": "processing_output",
              "required": true,
              "compatible_sources": [
                "DataLoad",
                "CradleDataLoading",
                "ProcessingStep"
              ],
              "data_type": "S3Uri",
              "description": "Raw validation data for preprocessing"
            }
          ],
          "outputs": [
            {
              "logical_name": "processed_data",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Processed validation data"
            }
          ]
        },
        "calibration": {
          "step_type": "TabularPreprocessing_Calibration",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "DATA",
              "dependency_type": "processing_output",
              "required": true,
              "compatible_sources": [
                "DataLoad",
                "CradleDataLoading",
                "ProcessingStep"
              ],
              "data_type": "S3Uri",
              "description": "Raw calibration data for preprocessing"
            }
          ],
          "outputs": [
            {
              "logical_name": "processed_data",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Processed calibration data for model evaluation"
            }
          ]
        },
        "generic": {
          "step_type": "TabularPreprocessing_Training",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "DATA",
              "dependency_type": "processing_output",
              "required": true,
              "compatible_sources": [
                "DataLoad",
                "CradleDataLoading",
                "ProcessingStep"
              ],
              "data_type": "S3Uri",
              "description": "Raw tabular data for preprocessing"
            }
          ],
          "outputs": [
            {
              "logical_name": "processed_data",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Processed tabular data with train/val/test splits"
            }
          ]
        }
      },
      "unified_dependencies": {
        "DATA": {
          "logical_name": "DATA",
          "dependency_type": "processing_output",
          "required": true,
          "compatible_sources": [
            "DataLoad",
            "CradleDataLoading",
            "ProcessingStep"
          ],
          "data_type": "S3Uri",
          "description": "Raw tabular data for preprocessing"
        }
      },
      "unified_outputs": {
        "processed_data": {
          "logical_name": "processed_data",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Processed tabular data with train/val/test splits"
        }
      },
      "dependency_sources": {
        "DATA": [
          "training",
          "testing",
          "validation",
          "calibration",
          "generic"
        ]
      },
      "output_sources": {
        "processed_data": [
          "training",
          "testing",
          "validation",
          "calibration",
          "generic"
        ]
      },
      "variant_count": 5
    }
  },
  "level3": {
    "passed": true,
    "issues": [],
    "specification": {
      "step_type": "TabularPreprocessing_Training",
      "node_type": "internal",
      "dependencies": [
        {
          "logical_name": "DATA",
          "dependency_type": "processing_output",
          "required": true,
          "compatible_sources": [
            "DataLoad",
            "CradleDataLoading",
            "ProcessingStep"
          ],
          "data_type": "S3Uri",
          "description": "Raw tabular data for preprocessing"
        }
      ],
      "outputs": [
        {
          "logical_name": "processed_data",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Processed tabular data with train/val/test splits"
        }
      ]
    }
  },
  "level4": {
    "passed": true,
    "issues": [
      {
        "severity": "WARNING",
        "category": "configuration_fields",
        "message": "Required configuration field not accessed in builder: label_name",
        "details": {
          "field_name": "label_name",
          "builder": "tabular_preprocessing"
        },
        "recommendation": "Access required field label_name in builder or make it optional"
      },
      {
        "severity": "INFO",
        "category": "required_field_validation",
        "message": "Builder has required fields but no explicit validation logic detected",
        "details": {
          "required_fields": [
            "label_name"
          ],
          "builder": "tabular_preprocessing"
        },
        "recommendation": "Consider adding explicit validation logic for required configuration fields"
      }
    ],
    "builder_analysis": {
      "config_accesses": [
        {
          "field_name": "job_type",
          "line_number": 65
        }
      ],
      "validation_calls": [],
      "default_assignments": [],
      "class_definitions": [
        {
          "class_name": "TabularPreprocessingStepBuilder",
          "line_number": 29
        }
      ],
      "method_definitions": [
        {
          "method_name": "__init__",
          "line_number": 37
        },
        {
          "method_name": "validate_configuration",
          "line_number": 98
        },
        {
          "method_name": "_create_processor",
          "line_number": 125
        },
        {
          "method_name": "_get_environment_variables",
          "line_number": 145
        },
        {
          "method_name": "_get_inputs",
          "line_number": 173
        },
        {
          "method_name": "_get_outputs",
          "line_number": 226
        },
        {
          "method_name": "_get_job_arguments",
          "line_number": 281
        },
        {
          "method_name": "create_step",
          "line_number": 301
        }
      ]
    },
    "config_analysis": {
      "class_name": "TabularPreprocessingConfig",
      "fields": {
        "label_name": {
          "type": "<class 'str'>",
          "required": true
        },
        "processing_entry_point": {
          "type": "<class 'str'>",
          "required": false
        },
        "job_type": {
          "type": "<class 'str'>",
          "required": false
        },
        "train_ratio": {
          "type": "<class 'float'>",
          "required": false
        },
        "test_val_ratio": {
          "type": "<class 'float'>",
          "required": false
        },
        "_full_script_path": {
          "type": "typing.Optional[str]",
          "required": false
        }
      },
      "required_fields": [
        "label_name"
      ],
      "optional_fields": [
        "_full_script_path",
        "job_type",
        "processing_entry_point",
        "test_val_ratio",
        "train_ratio"
      ],
      "default_values": {
        "aws_region": "<property>",
        "effective_instance_type": "<property>",
        "effective_source_dir": "<property>",
        "full_script_path": "<property>",
        "model_computed_fields": {},
        "model_config": {
          "arbitrary_types_allowed": true,
          "extra": "allow",
          "protected_namespaces": [],
          "validate_assignment": true
        },
        "model_extra": "<property>",
        "model_fields": {
          "author": "annotation=str required=True description='Author or owner of the pipeline.'",
          "bucket": "annotation=str required=True description='S3 bucket name for pipeline artifacts and data.'",
          "role": "annotation=str required=True description='IAM role for pipeline execution.'",
          "region": "annotation=str required=True description='Custom region code (NA, EU, FE) for internal logic.'",
          "service_name": "annotation=str required=True description='Service name for the pipeline.'",
          "pipeline_version": "annotation=str required=True description='Version string for the SageMaker Pipeline.'",
          "model_class": "annotation=str required=False default='xgboost' description='Model class (e.g., XGBoost, PyTorch).'",
          "current_date": "annotation=str required=False default_factory=<lambda> description='Current date, typically used for versioning or pathing.'",
          "framework_version": "annotation=str required=False default='2.1.0' description='Default framework version (e.g., PyTorch).'",
          "py_version": "annotation=str required=False default='py310' description='Default Python version.'",
          "source_dir": "annotation=Union[str, NoneType] required=False default=None description='Common source directory for scripts if applicable. Can be overridden by step configs.'",
          "processing_instance_count": "annotation=int required=False default=1 description='Instance count for processing jobs' metadata=[Ge(ge=1), Le(le=10)]",
          "processing_volume_size": "annotation=int required=False default=500 description='Volume size for processing jobs in GB' metadata=[Ge(ge=10), Le(le=1000)]",
          "processing_instance_type_large": "annotation=str required=False default='ml.m5.4xlarge' description='Large instance type for processing step.'",
          "processing_instance_type_small": "annotation=str required=False default='ml.m5.2xlarge' description='Small instance type for processing step.'",
          "use_large_processing_instance": "annotation=bool required=False default=False description='Set to True to use large instance type, False for small instance type.'",
          "processing_source_dir": "annotation=Union[str, NoneType] required=False default=None description='Source directory for processing scripts. Falls back to base source_dir if not provided.'",
          "processing_entry_point": "annotation=str required=False default='tabular_preprocessing.py' description='Relative path (within processing_source_dir) to the tabular preprocessing script.'",
          "processing_script_arguments": "annotation=Union[List[str], NoneType] required=False default=None description='Optional arguments for the processing script.'",
          "processing_framework_version": "annotation=str required=False default='1.2-1' description=\"Version of the scikit-learn framework to use in SageMaker Processing. Format: '<sklearn-version>-<build-number>'\"",
          "label_name": "annotation=str required=True description='Label field name for the target variable.'",
          "job_type": "annotation=str required=False default='training' description=\"One of ['training','validation','testing','calibration']\"",
          "train_ratio": "annotation=float required=False default=0.7 description=\"Fraction of data to allocate to the training set (only used if job_type=='training').\" metadata=[Ge(ge=0.0), Le(le=1.0)]",
          "test_val_ratio": "annotation=float required=False default=0.5 description=\"Fraction of the holdout to allocate to the test set vs. validation (only if job_type=='training').\" metadata=[Ge(ge=0.0), Le(le=1.0)]"
        },
        "model_fields_set": "<property>",
        "pipeline_description": "<property>",
        "pipeline_name": "<property>",
        "pipeline_s3_loc": "<property>",
        "script_contract": "<property>",
        "script_path": "<property>"
      }
    }
  },
  "overall_status": "PASSING",
  "metadata": {
    "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/tabular_preprocessing.py",
    "validation_timestamp": "2025-08-11T23:21:14.636806",
    "validator_version": "1.0.0"
  }
}