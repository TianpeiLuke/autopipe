{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost 3-Step Pipeline Runtime Test\n",
    "\n",
    "This notebook tests a complete XGBoost pipeline with 3 steps:\n",
    "1. XGBoost Training\n",
    "2. XGBoost Model Evaluation\n",
    "3. Model Calibration\n",
    "\n",
    "The pipeline follows the DAG: XGBoost Training ‚Üí XGBoost Model Eval ‚Üí Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Add cursus to path\n",
    "cursus_root = Path.cwd().parent.parent.parent\n",
    "sys.path.insert(0, str(cursus_root / 'src'))\n",
    "\n",
    "# Import cursus components\n",
    "from cursus.validation.runtime.core.pipeline_script_executor import PipelineScriptExecutor\n",
    "from cursus.validation.runtime.jupyter.notebook_interface import NotebookInterface\n",
    "from cursus.validation.runtime.data.data_flow_manager import DataFlowManager\n",
    "from cursus.steps.registry.step_names import StepNames\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Setup complete - All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Pipeline Definition and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test workspace\n",
    "test_workspace = Path.cwd()\n",
    "data_dir = test_workspace / 'data'\n",
    "configs_dir = test_workspace / 'configs'\n",
    "outputs_dir = test_workspace / 'outputs'\n",
    "workspace_dir = outputs_dir / 'workspace'\n",
    "logs_dir = outputs_dir / 'logs'\n",
    "results_dir = outputs_dir / 'results'\n",
    "\n",
    "# Ensure directories exist\n",
    "for directory in [data_dir, configs_dir, workspace_dir, logs_dir, results_dir]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pipeline definition\n",
    "pipeline_config = {\n",
    "    'pipeline_name': 'xgboost_3_step_test',\n",
    "    'steps': [\n",
    "        {\n",
    "            'step_name': StepNames.XGBOOST_TRAINING,\n",
    "            'step_id': 'xgb_train',\n",
    "            'dependencies': []\n",
    "        },\n",
    "        {\n",
    "            'step_name': StepNames.XGBOOST_MODEL_EVAL,\n",
    "            'step_id': 'xgb_eval',\n",
    "            'dependencies': ['xgb_train']\n",
    "        },\n",
    "        {\n",
    "            'step_name': StepNames.MODEL_CALIBRATION,\n",
    "            'step_id': 'model_calib',\n",
    "            'dependencies': ['xgb_eval']\n",
    "        }\n",
    "    ],\n",
    "    'workspace_dir': str(workspace_dir),\n",
    "    'logs_dir': str(logs_dir)\n",
    "}\n",
    "\n",
    "print(f\"üìã Pipeline configured with {len(pipeline_config['steps'])} steps\")\n",
    "print(f\"üìÅ Workspace: {workspace_dir}\")\n",
    "for step in pipeline_config['steps']:\n",
    "    deps = step['dependencies'] if step['dependencies'] else ['None']\n",
    "    print(f\"  - {step['step_id']}: {step['step_name']} (depends on: {', '.join(deps)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Test Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "\n",
    "# Create feature matrix\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "\n",
    "# Create target with some non-linear relationship\n",
    "y = (X[:, 0] * 2 + X[:, 1] * -1.5 + X[:, 2] * 0.8 + \n",
    "     np.sin(X[:, 3]) + np.random.normal(0, 0.1, n_samples))\n",
    "\n",
    "# Convert to binary classification\n",
    "y_binary = (y > np.median(y)).astype(int)\n",
    "\n",
    "# Create DataFrames\n",
    "train_df = pd.DataFrame(X, columns=feature_names)\n",
    "train_df['target'] = y_binary\n",
    "\n",
    "# Split for evaluation\n",
    "split_idx = int(0.8 * n_samples)\n",
    "train_data = train_df.iloc[:split_idx]\n",
    "eval_data = train_df.iloc[split_idx:]\n",
    "\n",
    "# Save datasets\n",
    "train_path = data_dir / 'train_data.csv'\n",
    "eval_path = data_dir / 'eval_data.csv'\n",
    "train_data.to_csv(train_path, index=False)\n",
    "eval_data.to_csv(eval_path, index=False)\n",
    "\n",
    "print(f\"üìä Generated synthetic dataset:\")\n",
    "print(f\"  - Training samples: {len(train_data)}\")\n",
    "print(f\"  - Evaluation samples: {len(eval_data)}\")\n",
    "print(f\"  - Features: {n_features}\")\n",
    "print(f\"  - Target distribution: {train_data['target'].value_counts().to_dict()}\")\n",
    "print(f\"  - Saved to: {train_path} and {eval_path}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìã Sample training data:\")\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Individual Step Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize runtime components\n",
    "script_executor = PipelineScriptExecutor()\n",
    "notebook_interface = NotebookInterface()\n",
    "data_flow_manager = DataFlowManager()\n",
    "\n",
    "# Test results storage\n",
    "step_test_results = {}\n",
    "\n",
    "def test_individual_step(step_config: Dict[str, Any], input_data_paths: Dict[str, str]) -> Dict[str, Any]:\n",
    "    \"\"\"Test an individual pipeline step\"\"\"\n",
    "    step_name = step_config['step_name']\n",
    "    step_id = step_config['step_id']\n",
    "    \n",
    "    print(f\"\\nüß™ Testing step: {step_id} ({step_name})\")\n",
    "    \n",
    "    # Create step workspace\n",
    "    step_workspace = workspace_dir / step_id\n",
    "    step_workspace.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Prepare step environment\n",
    "    step_env = {\n",
    "        'STEP_NAME': step_name,\n",
    "        'STEP_ID': step_id,\n",
    "        'WORKSPACE_DIR': str(step_workspace),\n",
    "        'INPUT_DATA_DIR': str(data_dir),\n",
    "        'OUTPUT_DATA_DIR': str(step_workspace / 'outputs'),\n",
    "        'LOGS_DIR': str(logs_dir)\n",
    "    }\n",
    "    \n",
    "    # Add input data paths to environment\n",
    "    for key, path in input_data_paths.items():\n",
    "        step_env[key] = path\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Simulate script execution (in real implementation, this would call actual scripts)\n",
    "        print(f\"  üìù Environment variables set: {len(step_env)} vars\")\n",
    "        print(f\"  üìÇ Workspace: {step_workspace}\")\n",
    "        \n",
    "        # Create mock outputs based on step type\n",
    "        output_dir = Path(step_env['OUTPUT_DATA_DIR'])\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if step_name == StepNames.XGBOOST_TRAINING:\n",
    "            # Mock model output\n",
    "            model_path = output_dir / 'model.json'\n",
    "            model_path.write_text('{\"model_type\": \"xgboost\", \"trained\": true}')\n",
    "            step_env['MODEL_OUTPUT_PATH'] = str(model_path)\n",
    "            \n",
    "        elif step_name == StepNames.XGBOOST_MODEL_EVAL:\n",
    "            # Mock evaluation results\n",
    "            eval_results = {\n",
    "                'accuracy': 0.85,\n",
    "                'precision': 0.82,\n",
    "                'recall': 0.88,\n",
    "                'f1_score': 0.85\n",
    "            }\n",
    "            eval_path = output_dir / 'evaluation_results.json'\n",
    "            eval_path.write_text(json.dumps(eval_results, indent=2))\n",
    "            step_env['EVAL_RESULTS_PATH'] = str(eval_path)\n",
    "            \n",
    "        elif step_name == StepNames.MODEL_CALIBRATION:\n",
    "            # Mock calibrated model\n",
    "            calib_model_path = output_dir / 'calibrated_model.json'\n",
    "            calib_model_path.write_text('{\"model_type\": \"calibrated_xgboost\", \"calibrated\": true}')\n",
    "            step_env['CALIBRATED_MODEL_PATH'] = str(calib_model_path)\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            'step_id': step_id,\n",
    "            'step_name': step_name,\n",
    "            'status': 'SUCCESS',\n",
    "            'execution_time': execution_time,\n",
    "            'workspace': str(step_workspace),\n",
    "            'environment': step_env,\n",
    "            'outputs': list(output_dir.glob('*')) if output_dir.exists() else []\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úÖ Step completed successfully in {execution_time:.2f}s\")\n",
    "        print(f\"  üìÑ Outputs: {len(result['outputs'])} files\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        result = {\n",
    "            'step_id': step_id,\n",
    "            'step_name': step_name,\n",
    "            'status': 'FAILED',\n",
    "            'execution_time': execution_time,\n",
    "            'error': str(e),\n",
    "            'workspace': str(step_workspace),\n",
    "            'environment': step_env\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚ùå Step failed after {execution_time:.2f}s: {e}\")\n",
    "        return result\n",
    "\n",
    "# Test each step individually\n",
    "print(\"üî¨ Starting individual step testing...\")\n",
    "\n",
    "# Test XGBoost Training\n",
    "step_test_results['xgb_train'] = test_individual_step(\n",
    "    pipeline_config['steps'][0],\n",
    "    {'TRAIN_DATA_PATH': str(train_path)}\n",
    ")\n",
    "\n",
    "# Test XGBoost Model Eval (depends on training output)\n",
    "model_path = step_test_results['xgb_train']['environment'].get('MODEL_OUTPUT_PATH', '')\n",
    "step_test_results['xgb_eval'] = test_individual_step(\n",
    "    pipeline_config['steps'][1],\n",
    "    {\n",
    "        'MODEL_PATH': model_path,\n",
    "        'EVAL_DATA_PATH': str(eval_path)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test Model Calibration (depends on evaluation output)\n",
    "eval_results_path = step_test_results['xgb_eval']['environment'].get('EVAL_RESULTS_PATH', '')\n",
    "step_test_results['model_calib'] = test_individual_step(\n",
    "    pipeline_config['steps'][2],\n",
    "    {\n",
    "        'MODEL_PATH': model_path,\n",
    "        'EVAL_RESULTS_PATH': eval_results_path,\n",
    "        'CALIB_DATA_PATH': str(eval_path)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Individual step testing summary:\")\n",
    "for step_id, result in step_test_results.items():\n",
    "    status_icon = \"‚úÖ\" if result['status'] == 'SUCCESS' else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {step_id}: {result['status']} ({result['execution_time']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: End-to-End Pipeline Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline_end_to_end(pipeline_config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Execute the complete pipeline end-to-end\"\"\"\n",
    "    print(\"üöÄ Starting end-to-end pipeline execution...\")\n",
    "    \n",
    "    pipeline_start_time = time.time()\n",
    "    pipeline_results = {\n",
    "        'pipeline_name': pipeline_config['pipeline_name'],\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "        'steps': [],\n",
    "        'data_flow': [],\n",
    "        'overall_status': 'RUNNING'\n",
    "    }\n",
    "    \n",
    "    # Track data flow between steps\n",
    "    step_outputs = {}\n",
    "    \n",
    "    try:\n",
    "        for i, step_config in enumerate(pipeline_config['steps']):\n",
    "            step_id = step_config['step_id']\n",
    "            step_name = step_config['step_name']\n",
    "            dependencies = step_config['dependencies']\n",
    "            \n",
    "            print(f\"\\nüìã Executing step {i+1}/{len(pipeline_config['steps'])}: {step_id}\")\n",
    "            \n",
    "            # Prepare input data based on dependencies\n",
    "            input_data_paths = {}\n",
    "            \n",
    "            if step_name == StepNames.XGBOOST_TRAINING:\n",
    "                input_data_paths['TRAIN_DATA_PATH'] = str(train_path)\n",
    "                \n",
    "            elif step_name == StepNames.XGBOOST_MODEL_EVAL:\n",
    "                # Get model from training step\n",
    "                if 'xgb_train' in step_outputs:\n",
    "                    input_data_paths['MODEL_PATH'] = step_outputs['xgb_train']['model_path']\n",
    "                input_data_paths['EVAL_DATA_PATH'] = str(eval_path)\n",
    "                \n",
    "            elif step_name == StepNames.MODEL_CALIBRATION:\n",
    "                # Get model and evaluation results from previous steps\n",
    "                if 'xgb_train' in step_outputs:\n",
    "                    input_data_paths['MODEL_PATH'] = step_outputs['xgb_train']['model_path']\n",
    "                if 'xgb_eval' in step_outputs:\n",
    "                    input_data_paths['EVAL_RESULTS_PATH'] = step_outputs['xgb_eval']['eval_results_path']\n",
    "                input_data_paths['CALIB_DATA_PATH'] = str(eval_path)\n",
    "            \n",
    "            # Execute step\n",
    "            step_result = test_individual_step(step_config, input_data_paths)\n",
    "            pipeline_results['steps'].append(step_result)\n",
    "            \n",
    "            # Track outputs for next steps\n",
    "            if step_result['status'] == 'SUCCESS':\n",
    "                if step_name == StepNames.XGBOOST_TRAINING:\n",
    "                    step_outputs[step_id] = {\n",
    "                        'model_path': step_result['environment'].get('MODEL_OUTPUT_PATH')\n",
    "                    }\n",
    "                elif step_name == StepNames.XGBOOST_MODEL_EVAL:\n",
    "                    step_outputs[step_id] = {\n",
    "                        'eval_results_path': step_result['environment'].get('EVAL_RESULTS_PATH')\n",
    "                    }\n",
    "                elif step_name == StepNames.MODEL_CALIBRATION:\n",
    "                    step_outputs[step_id] = {\n",
    "                        'calibrated_model_path': step_result['environment'].get('CALIBRATED_MODEL_PATH')\n",
    "                    }\n",
    "                \n",
    "                # Record data flow\n",
    "                data_flow_entry = {\n",
    "                    'from_step': dependencies[0] if dependencies else 'INPUT',\n",
    "                    'to_step': step_id,\n",
    "                    'data_paths': input_data_paths,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                pipeline_results['data_flow'].append(data_flow_entry)\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ùå Pipeline failed at step: {step_id}\")\n",
    "                pipeline_results['overall_status'] = 'FAILED'\n",
    "                pipeline_results['failed_step'] = step_id\n",
    "                break\n",
    "        \n",
    "        if pipeline_results['overall_status'] != 'FAILED':\n",
    "            pipeline_results['overall_status'] = 'SUCCESS'\n",
    "            \n",
    "    except Exception as e:\n",
    "        pipeline_results['overall_status'] = 'FAILED'\n",
    "        pipeline_results['error'] = str(e)\n",
    "        print(f\"‚ùå Pipeline execution failed: {e}\")\n",
    "    \n",
    "    pipeline_execution_time = time.time() - pipeline_start_time\n",
    "    pipeline_results['execution_time'] = pipeline_execution_time\n",
    "    pipeline_results['end_time'] = datetime.now().isoformat()\n",
    "    \n",
    "    return pipeline_results\n",
    "\n",
    "# Execute end-to-end pipeline\n",
    "pipeline_results = execute_pipeline_end_to_end(pipeline_config)\n",
    "\n",
    "# Save results\n",
    "results_file = results_dir / f\"pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(pipeline_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüéØ End-to-end pipeline execution completed!\")\n",
    "print(f\"üìä Overall Status: {pipeline_results['overall_status']}\")\n",
    "print(f\"‚è±Ô∏è  Total Execution Time: {pipeline_results['execution_time']:.2f}s\")\n",
    "print(f\"üìÅ Results saved to: {results_file}\")\n",
    "\n",
    "if pipeline_results['overall_status'] == 'SUCCESS':\n",
    "    print(f\"‚úÖ All {len(pipeline_results['steps'])} steps completed successfully\")\n",
    "    print(f\"üîÑ Data flow tracked: {len(pipeline_results['data_flow'])} transitions\")\n",
    "else:\n",
    "    print(f\"‚ùå Pipeline failed at step: {pipeline_results.get('failed_step', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Performance Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performance metrics\n",
    "step_names = [step['step_id'] for step in pipeline_results['steps']]\n",
    "execution_times = [step['execution_time'] for step in pipeline_results['steps']]\n",
    "success_status = [step['status'] == 'SUCCESS' for step in pipeline_results['steps']]\n",
    "\n",
    "# Mock additional metrics for visualization\n",
    "memory_usage = [np.random.uniform(50, 200) for _ in step_names]  # MB\n",
    "cpu_usage = [np.random.uniform(20, 80) for _ in step_names]      # %\n",
    "\n",
    "print(\"üìà Performance Analysis:\")\n",
    "print(f\"  Total Pipeline Time: {pipeline_results['execution_time']:.2f}s\")\n",
    "print(f\"  Average Step Time: {np.mean(execution_times):.2f}s\")\n",
    "print(f\"  Success Rate: {sum(success_status)}/{len(success_status)} steps\")\n",
    "\n",
    "# Create performance visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('XGBoost Pipeline Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Execution time chart\n",
    "axes[0, 0].bar(step_names, execution_times, color='skyblue', alpha=0.7)\n",
    "axes[0, 0].set_title('Execution Time by Step')\n",
    "axes[0, 0].set_ylabel('Time (seconds)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Memory usage chart\n",
    "axes[0, 1].bar(step_names, memory_usage, color='lightgreen', alpha=0.7)\n",
    "axes[0, 1].set_title('Memory Usage by Step')\n",
    "axes[0, 1].set_ylabel('Memory (MB)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pipeline timeline\n",
    "cumulative_time = [sum(execution_times[:i+1]) for i in range(len(execution_times))]\n",
    "axes[1, 0].plot(step_names, cumulative_time, marker='o', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_title('Pipeline Timeline')\n",
    "axes[1, 0].set_ylabel('Cumulative Time (s)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Success rate\n",
    "success_count = [1 if success else 0 for success in success_status]\n",
    "colors = ['green' if success else 'red' for success in success_status]\n",
    "axes[1, 1].bar(step_names, success_count, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_title('Success Rate')\n",
    "axes[1, 1].set_ylabel('Success (1=Pass, 0=Fail)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].set_ylim(0, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary table\n",
    "performance_df = pd.DataFrame({\n",
    "    'Step': step_names,\n",
    "    'Execution Time (s)': execution_times,\n",
    "    'Memory Usage (MB)': memory_usage,\n",
    "    'CPU Usage (%)': cpu_usage,\n",
    "    'Status': ['‚úÖ SUCCESS' if s else '‚ùå FAILED' for s in success_status]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Performance Summary Table:\")\n",
    "print(performance_df.to_string(index=False))\n",
    "\n",
    "# Data flow visualization\n",
    "print(\"\\nüîÑ Data Flow Analysis:\")\n",
    "for i, flow in enumerate(pipeline_results['data_flow']):\n",
    "    print(f\"  {i+1}. {flow['from_step']} ‚Üí {flow['to_step']}\")\n",
    "    for key, path in flow['data_paths'].items():\n",
    "        print(f\"     {key}: {Path(path).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Error Handling and Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_scenarios():\n",
    "    \"\"\"Test various error scenarios and edge cases\"\"\"\n",
    "    print(\"üß™ Testing Error Scenarios and Edge Cases...\")\n",
    "    \n",
    "    error_test_results = []\n",
    "    \n",
    "    # Test 1: Missing input data\n",
    "    print(\"\\n1Ô∏è‚É£ Testing missing input data scenario...\")\n",
    "    try:\n",
    "        missing_data_config = {\n",
    "            'step_name': StepNames.XGBOOST_TRAINING,\n",
    "            'step_id': 'xgb_train_missing_data',\n",
    "            'dependencies': []\n",
    "        }\n",
    "        \n",
    "        # Test with non-existent data path\n",
    "        result = test_individual_step(\n",
    "            missing_data_config,\n",
    "            {'TRAIN_DATA_PATH': '/non/existent/path.csv'}\n",
    "        )\n",
    "        error_test_results.append(('missing_input_data', result['status']))\n",
    "        print(f\"  Result: {result['status']} - {result.get('error', 'No error')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_test_results.append(('missing_input_data', 'EXCEPTION'))\n",
    "        print(f\"  Exception caught: {e}\")\n",
    "    \n",
    "    # Test 2: Invalid step dependencies\n",
    "    print(\"\\n2Ô∏è‚É£ Testing invalid step dependencies...\")\n",
    "    try:\n",
    "        invalid_pipeline = {\n",
    "            'pipeline_name': 'invalid_dependency_test',\n",
    "            'steps': [\n",
    "                {\n",
    "                    'step_name': StepNames.XGBOOST_MODEL_EVAL,\n",
    "                    'step_id': 'eval_without_model',\n",
    "                    'dependencies': ['non_existent_step']\n",
    "                }\n",
    "            ],\n",
    "            'workspace_dir': str(workspace_dir),\n",
    "            'logs_dir': str(logs_dir)\n",
    "        }\n",
    "        \n",
    "        # This should fail due to missing dependency\n",
    "        result = execute_pipeline_end_to_end(invalid_pipeline)\n",
    "        error_test_results.append(('invalid_dependencies', result['overall_status']))\n",
    "        print(f\"  Result: {result['overall_status']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_test_results.append(('invalid_dependencies', 'EXCEPTION'))\n",
    "        print(f\"  Exception caught: {e}\")\n",
    "    \n",
    "    # Test 3: Corrupted data format\n",
    "    print(\"\\n3Ô∏è‚É£ Testing corrupted data format...\")\n",
    "    try:\n",
    "        # Create corrupted data file\n",
    "        corrupted_path = data_dir / 'corrupted_data.csv'\n",
    "        corrupted_path.write_text('invalid,csv,format\\nwith,missing,columns\\n')\n",
    "        \n",
    "        corrupted_config = {\n",
    "            'step_name': StepNames.XGBOOST_TRAINING,\n",
    "            'step_id': 'xgb_train_corrupted',\n",
    "            'dependencies': []\n",
    "        }\n",
    "        \n",
    "        result = test_individual_step(\n",
    "            corrupted_config,\n",
    "            {'TRAIN_DATA_PATH': str(corrupted_path)}\n",
    "        )\n",
    "        error_test_results.append(('corrupted_data', result['status']))\n",
    "        print(f\"  Result: {result['status']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_test_results.append(('corrupted_data', 'EXCEPTION'))\n",
    "        print(f\"  Exception caught: {e}\")\n",
    "    \n",
    "    # Test 4: Resource constraints (simulated)\n",
    "    print(\"\\n4Ô∏è‚É£ Testing resource constraints...\")\n",
    "    try:\n",
    "        # Simulate memory constraint by creating large dataset\n",
    "        large_data = pd.DataFrame({\n",
    "            f'feature_{i}': np.random.randn(10000) for i in range(100)\n",
    "        })\n",
    "        large_data['target'] = np.random.randint(0, 2, 10000)\n",
    "        \n",
    "        large_data_path = data_dir / 'large_dataset.csv'\n",
    "        large_data.to_csv(large_data_path, index=False)\n",
    "        \n",
    "        resource_config = {\n",
    "            'step_name': StepNames.XGBOOST_TRAINING,\n",
    "            'step_id': 'xgb_train_large',\n",
    "            'dependencies': []\n",
    "        }\n",
    "        \n",
    "        result = test_individual_step(\n",
    "            resource_config,\n",
    "            {'TRAIN_DATA_PATH': str(large_data_path)}\n",
    "        )\n",
    "        error_test_results.append(('resource_constraints', result['status']))\n",
    "        print(f\"  Result: {result['status']} (Large dataset: {large_data.shape})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_test_results.append(('resource_constraints', 'EXCEPTION'))\n",
    "        print(f\"  Exception caught: {e}\")\n",
    "    \n",
    "    return error_test_results\n",
    "\n",
    "# Run error scenario tests\n",
    "error_results = test_error_scenarios()\n",
    "\n",
    "print(\"\\nüìã Error Scenario Test Summary:\")\n",
    "for test_name, status in error_results:\n",
    "    status_icon = \"‚úÖ\" if status in ['SUCCESS', 'FAILED'] else \"‚ö†Ô∏è\"\n",
    "    print(f\"  {status_icon} {test_name}: {status}\")\n",
    "\n",
    "# Test edge cases\n",
    "print(\"\\nüîç Testing Edge Cases...\")\n",
    "\n",
    "# Edge case 1: Empty dataset\n",
    "empty_df = pd.DataFrame(columns=['feature_0', 'target'])\n",
    "empty_path = data_dir / 'empty_dataset.csv'\n",
    "empty_df.to_csv(empty_path, index=False)\n",
    "\n",
    "print(f\"\\nüìä Edge Case - Empty Dataset: {empty_df.shape}\")\n",
    "\n",
    "# Edge case 2: Single sample dataset\n",
    "single_df = pd.DataFrame({\n",
    "    'feature_0': [1.0],\n",
    "    'target': [1]\n",
    "})\n",
    "single_path = data_dir / 'single_sample.csv'\n",
    "single_df.to_csv(single_path, index=False)\n",
    "\n",
    "print(f\"üìä Edge Case - Single Sample: {single_df.shape}\")\n",
    "\n",
    "# Edge case 3: All same target values\n",
    "uniform_df = pd.DataFrame({\n",
    "    f'feature_{i}': np.random.randn(100) for i in range(5)\n",
    "})\n",
    "uniform_df['target'] = 1  # All same class\n",
    "uniform_path = data_dir / 'uniform_target.csv'\n",
    "uniform_df.to_csv(uniform_path, index=False)\n",
    "\n",
    "print(f\"üìä Edge Case - Uniform Target: {uniform_df.shape}, unique targets: {uniform_df['target'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Results Summary and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_report():\n",
    "    \"\"\"Generate a comprehensive test report\"\"\"\n",
    "    print(\"üìã Generating Comprehensive Test Report...\")\n",
    "    \n",
    "    report = {\n",
    "        'test_execution_summary': {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'pipeline_name': pipeline_config['pipeline_name'],\n",
    "            'total_steps': len(pipeline_config['steps']),\n",
    "            'test_workspace': str(test_workspace)\n",
    "        },\n",
    "        'individual_step_results': step_test_results,\n",
    "        'end_to_end_results': pipeline_results,\n",
    "        'error_scenario_results': dict(error_results),\n",
    "        'performance_metrics': {\n",
    "            'total_execution_time': pipeline_results['execution_time'],\n",
    "            'average_step_time': np.mean([step['execution_time'] for step in pipeline_results['steps']]),\n",
    "            'success_rate': sum([step['status'] == 'SUCCESS' for step in pipeline_results['steps']]) / len(pipeline_results['steps']),\n",
    "            'data_flow_transitions': len(pipeline_results['data_flow'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save comprehensive report\n",
    "    report_file = results_dir / f\"comprehensive_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    \n",
    "    return report, report_file\n",
    "\n",
    "# Generate final report\n",
    "final_report, report_path = generate_comprehensive_report()\n",
    "\n",
    "print(\"\\nüéØ FINAL TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìä Pipeline Overview:\")\n",
    "print(f\"  ‚Ä¢ Pipeline Name: {final_report['test_execution_summary']['pipeline_name']}\")\n",
    "print(f\"  ‚Ä¢ Total Steps: {final_report['test_execution_summary']['total_steps']}\")\n",
    "print(f\"  ‚Ä¢ Test Workspace: {final_report['test_execution_summary']['test_workspace']}\")\n",
    "\n",
    "print(f\"\\n‚ö° Performance Metrics:\")\n",
    "perf = final_report['performance_metrics']\n",
    "print(f\"  ‚Ä¢ Total Execution Time: {perf['total_execution_time']:.2f}s\")\n",
    "print(f\"  ‚Ä¢ Average Step Time: {perf['average_step_time']:.2f}s\")\n",
    "print(f\"  ‚Ä¢ Success Rate: {perf['success_rate']:.1%}\")\n",
    "print(f\"  ‚Ä¢ Data Flow Transitions: {perf['data_flow_transitions']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Individual Step Results:\")\n",
    "for step_id, result in final_report['individual_step_results'].items():\n",
    "    status_icon = \"‚úÖ\" if result['status'] == 'SUCCESS' else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {step_id}: {result['status']} ({result['execution_time']:.2f}s)\")\n",
    "\n",
    "print(f\"\\nüöÄ End-to-End Pipeline:\")\n",
    "e2e_status = final_report['end_to_end_results']['overall_status']\n",
    "e2e_icon = \"‚úÖ\" if e2e_status == 'SUCCESS' else \"‚ùå\"\n",
    "print(f\"  {e2e_icon} Overall Status: {e2e_status}\")\n",
    "print(f\"  ‚è±Ô∏è  Total Time: {final_report['end_to_end_results']['execution_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\nüß™ Error Scenario Testing:\")\n",
    "for test_name, status in final_report['error_scenario_results'].items():\n",
    "    status_icon = \"‚úÖ\" if status in ['SUCCESS', 'FAILED'] else \"‚ö†Ô∏è\"\n",
    "    print(f\"  {status_icon} {test_name}: {status}\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"  ‚Ä¢ Test Data: {data_dir}\")\n",
    "print(f\"  ‚Ä¢ Workspace: {workspace_dir}\")\n",
    "print(f\"  ‚Ä¢ Results: {results_dir}\")\n",
    "print(f\"  ‚Ä¢ Comprehensive Report: {report_path}\")\n",
    "\n",
    "print(f\"\\nüéâ Test Execution Complete!\")\n",
    "print(f\"üìã This notebook successfully tested the XGBoost 3-step pipeline using the Cursus Pipeline Runtime Testing System.\")\n",
    "print(f\"üîç All components were validated: script execution, data flow, error handling, and performance monitoring.\")\n",
    "\n",
    "# Display final workspace structure\n",
    "print(f\"\\nüìÇ Final Workspace Structure:\")\n",
    "for root, dirs, files in os.walk(test_workspace):\n",
    "    level = root.replace(str(test_workspace), '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Limit to first 5 files per directory\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"{subindent}... and {len(files) - 5} more files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
