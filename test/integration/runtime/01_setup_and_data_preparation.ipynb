{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Pipeline Test - Setup and Data Preparation\n",
    "\n",
    "This notebook handles the initial setup and synthetic data generation for the XGBoost 3-step pipeline test.\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. XGBoost Training\n",
    "2. XGBoost Model Evaluation  \n",
    "3. Model Calibration\n",
    "\n",
    "**This notebook covers:**\n",
    "- Environment setup and imports\n",
    "- Synthetic dataset generation\n",
    "- Data exploration and visualization\n",
    "- Save datasets for use in other notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add cursus to path\n",
    "sys.path.append(str(Path.cwd().parent.parent.parent / 'src'))\n",
    "\n",
    "# Import Cursus components\n",
    "try:\n",
    "    from cursus.validation.runtime.jupyter.notebook_interface import NotebookInterface\n",
    "    from cursus.validation.runtime.core.data_flow_manager import DataFlowManager\n",
    "    from cursus.steps.registry.step_names import STEP_NAMES\n",
    "    print(\"✓ Successfully imported Cursus components\")\n",
    "    cursus_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"⚠ Import error: {e}\")\n",
    "    print(\"Continuing with mock implementations for testing...\")\n",
    "    cursus_available = False\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Setup completed at {datetime.now()}\")\n",
    "print(f\"Cursus components available: {cursus_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Directory Structure Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory structure\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "CONFIG_DIR = BASE_DIR / 'configs'\n",
    "OUTPUTS_DIR = BASE_DIR / 'outputs'\n",
    "WORKSPACE_DIR = OUTPUTS_DIR / 'workspace'\n",
    "LOGS_DIR = OUTPUTS_DIR / 'logs'\n",
    "RESULTS_DIR = OUTPUTS_DIR / 'results'\n",
    "\n",
    "# Create directories\n",
    "directories = [DATA_DIR, CONFIG_DIR, OUTPUTS_DIR, WORKSPACE_DIR, LOGS_DIR, RESULTS_DIR]\n",
    "for directory in directories:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✓ Created directory: {directory}\")\n",
    "\n",
    "print(\"\\nDirectory structure created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_dataset(n_samples=1000, n_features=10, random_state=42, dataset_name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic dataset for XGBoost training and evaluation.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "        n_features: Number of features\n",
    "        random_state: Random seed for reproducibility\n",
    "        dataset_name: Name for logging purposes\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_df, y_df) - Features and target DataFrames\n",
    "    \"\"\"\n",
    "    print(f\"Generating {dataset_name} with {n_samples} samples and {n_features} features...\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate features with different distributions\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Add some correlated features\n",
    "    if n_features >= 3:\n",
    "        X[:, 2] = 0.5 * X[:, 0] + 0.3 * X[:, 1] + 0.2 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Generate target with non-linear relationships\n",
    "    y_continuous = (\n",
    "        2.0 * X[:, 0] + \n",
    "        1.5 * X[:, 1] - \n",
    "        0.8 * X[:, 2] + \n",
    "        0.5 * X[:, 0] * X[:, 1] +  # Interaction term\n",
    "        0.3 * np.sin(X[:, 0]) +    # Non-linear term\n",
    "        np.random.normal(0, 0.1, n_samples)  # Noise\n",
    "    )\n",
    "    \n",
    "    # Convert to binary classification (balanced)\n",
    "    y_binary = (y_continuous > np.median(y_continuous)).astype(int)\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "    \n",
    "    # Create DataFrames\n",
    "    X_df = pd.DataFrame(X, columns=feature_names)\n",
    "    y_df = pd.DataFrame({'target': y_binary})\n",
    "    \n",
    "    print(f\"✓ Generated {dataset_name}: X shape {X_df.shape}, y distribution: {np.bincount(y_binary)}\")\n",
    "    \n",
    "    return X_df, y_df\n",
    "\n",
    "# Generate training and evaluation datasets\n",
    "print(\"=\" * 50)\n",
    "print(\"GENERATING SYNTHETIC DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "X_train, y_train = generate_synthetic_dataset(\n",
    "    n_samples=800, \n",
    "    n_features=10, \n",
    "    random_state=42, \n",
    "    dataset_name=\"training dataset\"\n",
    ")\n",
    "\n",
    "X_eval, y_eval = generate_synthetic_dataset(\n",
    "    n_samples=200, \n",
    "    n_features=10, \n",
    "    random_state=123, \n",
    "    dataset_name=\"evaluation dataset\"\n",
    ")\n",
    "\n",
    "print(\"\\nDataset generation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for exploration\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "eval_data = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "print(\"TRAINING DATA SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Shape: {train_data.shape}\")\n",
    "print(f\"Target distribution: {train_data['target'].value_counts().to_dict()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(train_data.head())\n",
    "\n",
    "print(\"\\nEVALUATION DATA SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Shape: {eval_data.shape}\")\n",
    "print(f\"Target distribution: {eval_data['target'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Synthetic Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Feature distributions\n",
    "ax1 = axes[0, 0]\n",
    "train_data[['feature_0', 'feature_1', 'feature_2']].hist(ax=ax1, bins=20, alpha=0.7)\n",
    "ax1.set_title('Feature Distributions (Training)')\n",
    "ax1.legend(['Feature 0', 'Feature 1', 'Feature 2'])\n",
    "\n",
    "# 2. Target distribution\n",
    "ax2 = axes[0, 1]\n",
    "train_target_counts = train_data['target'].value_counts()\n",
    "ax2.bar(train_target_counts.index, train_target_counts.values, color=['lightcoral', 'lightblue'])\n",
    "ax2.set_title('Target Distribution (Training)')\n",
    "ax2.set_xlabel('Target Class')\n",
    "ax2.set_ylabel('Count')\n",
    "for i, v in enumerate(train_target_counts.values):\n",
    "    ax2.text(i, v + 5, str(v), ha='center', va='bottom')\n",
    "\n",
    "# 3. Feature correlation heatmap\n",
    "ax3 = axes[1, 0]\n",
    "correlation_matrix = train_data.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=ax3, fmt='.2f')\n",
    "ax3.set_title('Feature Correlation Matrix')\n",
    "\n",
    "# 4. Feature vs Target relationship\n",
    "ax4 = axes[1, 1]\n",
    "for target_class in [0, 1]:\n",
    "    subset = train_data[train_data['target'] == target_class]\n",
    "    ax4.scatter(subset['feature_0'], subset['feature_1'], \n",
    "               alpha=0.6, label=f'Class {target_class}')\n",
    "ax4.set_xlabel('Feature 0')\n",
    "ax4.set_ylabel('Feature 1')\n",
    "ax4.set_title('Feature 0 vs Feature 1 by Target Class')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Data exploration visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets to CSV files\n",
    "train_data_path = DATA_DIR / 'train_data.csv'\n",
    "eval_data_path = DATA_DIR / 'eval_data.csv'\n",
    "\n",
    "train_data.to_csv(train_data_path, index=False)\n",
    "eval_data.to_csv(eval_data_path, index=False)\n",
    "\n",
    "print(f\"✓ Training data saved: {train_data_path}\")\n",
    "print(f\"✓ Evaluation data saved: {eval_data_path}\")\n",
    "\n",
    "# Save dataset metadata\n",
    "metadata = {\n",
    "    'creation_timestamp': datetime.now().isoformat(),\n",
    "    'training_data': {\n",
    "        'path': str(train_data_path),\n",
    "        'shape': train_data.shape,\n",
    "        'target_distribution': train_data['target'].value_counts().to_dict(),\n",
    "        'features': list(X_train.columns)\n",
    "    },\n",
    "    'evaluation_data': {\n",
    "        'path': str(eval_data_path),\n",
    "        'shape': eval_data.shape,\n",
    "        'target_distribution': eval_data['target'].value_counts().to_dict(),\n",
    "        'features': list(X_eval.columns)\n",
    "    },\n",
    "    'generation_parameters': {\n",
    "        'train_samples': 800,\n",
    "        'eval_samples': 200,\n",
    "        'n_features': 10,\n",
    "        'train_random_state': 42,\n",
    "        'eval_random_state': 123\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = DATA_DIR / 'dataset_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Dataset metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SETUP AND DATA PREPARATION COMPLETED\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training data: {train_data.shape[0]} samples, {train_data.shape[1]-1} features\")\n",
    "print(f\"Evaluation data: {eval_data.shape[0]} samples, {eval_data.shape[1]-1} features\")\n",
    "print(f\"Data saved to: {DATA_DIR}\")\n",
    "print(\"\\nReady for pipeline configuration and testing!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
