{
  "script_name": "package",
  "level1": {
    "passed": true,
    "issues": [
      {
        "severity": "WARNING",
        "category": "path_usage",
        "message": "Contract declares path not used in script: /opt/ml/processing/input/calibration",
        "details": {
          "path": "/opt/ml/processing/input/calibration",
          "script": "package"
        },
        "recommendation": "Either use path /opt/ml/processing/input/calibration in script or remove from contract"
      },
      {
        "severity": "INFO",
        "category": "file_operations",
        "message": "Contract declares input not read by script: /opt/ml/processing/input/calibration",
        "details": {
          "path": "/opt/ml/processing/input/calibration",
          "operation": "read",
          "script": "package"
        },
        "recommendation": "Either read /opt/ml/processing/input/calibration in script or remove from contract inputs"
      }
    ],
    "script_analysis": {
      "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/package.py",
      "path_references": [
        "path='/opt/ml/processing/input/model' line_number=18 context='\\n# Constants\\n>>> MODEL_PATH = Path(\"/opt/ml/processing/input/model\")\\nSCRIPT_PATH = Path(\"/opt/ml/processing/input/script\")\\nOUTPUT_PATH = Path(\"/opt/ml/processing/output\")' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/script' line_number=19 context='# Constants\\nMODEL_PATH = Path(\"/opt/ml/processing/input/model\")\\n>>> SCRIPT_PATH = Path(\"/opt/ml/processing/input/script\")\\nOUTPUT_PATH = Path(\"/opt/ml/processing/output\")\\nWORKING_DIRECTORY = Path(\"/tmp/mims_packaging_directory\")' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output' line_number=20 context='MODEL_PATH = Path(\"/opt/ml/processing/input/model\")\\nSCRIPT_PATH = Path(\"/opt/ml/processing/input/script\")\\n>>> OUTPUT_PATH = Path(\"/opt/ml/processing/output\")\\nWORKING_DIRECTORY = Path(\"/tmp/mims_packaging_directory\")\\nCODE_DIRECTORY = WORKING_DIRECTORY / \"code\"' is_hardcoded=True construction_method=None",
        "path='/tmp/mims_packaging_directory' line_number=21 context='SCRIPT_PATH = Path(\"/opt/ml/processing/input/script\")\\nOUTPUT_PATH = Path(\"/opt/ml/processing/output\")\\n>>> WORKING_DIRECTORY = Path(\"/tmp/mims_packaging_directory\")\\nCODE_DIRECTORY = WORKING_DIRECTORY / \"code\"\\n' is_hardcoded=True construction_method=None",
        "path='Ensure a directory exists, creating it if necessary.' line_number=26 context='\\ndef ensure_directory(directory: Path):\\n>>>     \"\"\"Ensure a directory exists, creating it if necessary.\"\"\"\\n    try:\\n        directory.mkdir(parents=True, exist_ok=True)' is_hardcoded=True construction_method=None",
        "path='Check if a file exists and log its details.' line_number=38 context='    \\ndef check_file_exists(path: Path, description: str) -> bool:\\n>>>     \"\"\"Check if a file exists and log its details.\"\"\"\\n    exists = path.exists() and path.is_file()\\n    try:' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=46 context='            logger.info(f\"{description}:\")\\n            logger.info(f\"  Path: {path}\")\\n>>>             logger.info(f\"  Size: {size_mb:.2f}MB\")\\n            logger.info(f\"  Permissions: {oct(stats.st_mode)[-3:]}\")\\n            logger.info(f\"  Last modified: {stats.st_mtime}\")' is_hardcoded=True construction_method=None",
        "path='List and log the contents of a directory.' line_number=58 context='\\ndef list_directory_contents(path: Path, description: str):\\n>>>     \"\"\"List and log the contents of a directory.\"\"\"\\n    logger.info(f\"\\\\n{\\'=\\'*20} Contents of {description} {\\'=\\'*20}\")\\n    logger.info(f\"Path: {path}\")' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=83 context='                    total_size += size_mb\\n                    file_count += 1\\n>>>                     logger.info(f\"{indent}\ud83d\udcc4 {item.name} ({size_mb:.2f}MB)\")\\n                elif item.is_dir():\\n                    dir_count += 1' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=93 context='        logger.info(f\"  Total files: {file_count}\")\\n        logger.info(f\"  Total directories: {dir_count}\")\\n>>>         logger.info(f\"  Total size: {total_size:.2f}MB\")\\n        \\n    except Exception as e:' is_hardcoded=True construction_method=None",
        "path='Copy a file and log the operation, ensuring destination directory exists.' line_number=100 context='    \\ndef copy_file_robust(src: Path, dst: Path):\\n>>>     \"\"\"Copy a file and log the operation, ensuring destination directory exists.\"\"\"\\n    logger.info(f\"\\\\nAttempting to copy file:\")\\n    logger.info(f\"  From: {src}\")' is_hardcoded=True construction_method=None",
        "path='Source file does not exist or is not a file. Skipping copy.' line_number=106 context='    \\n    if not check_file_exists(src, \"Source file for copy\"):\\n>>>         logger.warning(\"Source file does not exist or is not a file. Skipping copy.\")\\n        return False\\n    ' is_hardcoded=True construction_method=None",
        "path='Recursively copy scripts from source to destination.' line_number=124 context='\\ndef copy_scripts(src_dir: Path, dst_dir: Path):\\n>>>     \"\"\"Recursively copy scripts from source to destination.\"\"\"\\n    logger.info(f\"\\\\n{\\'=\\'*20} Copying Scripts {\\'=\\'*20}\")\\n    logger.info(f\"From: {src_dir}\")' is_hardcoded=True construction_method=None",
        "path='Source scripts directory does not exist or is not a directory. Skipping script copy.' line_number=132 context='\\n    if not src_dir.exists() or not src_dir.is_dir():\\n>>>         logger.warning(\"Source scripts directory does not exist or is not a directory. Skipping script copy.\")\\n        return\\n' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=150 context='    logger.info(f\"\\\\nScript copying summary:\")\\n    logger.info(f\"  Files copied: {files_copied}\")\\n>>>     logger.info(f\"  Total size: {total_size_mb:.2f}MB\")\\n    \\n    list_directory_contents(dst_dir, \"Destination scripts directory\")' is_hardcoded=True construction_method=None",
        "path='Extract a tar file to the specified path.' line_number=156 context='\\ndef extract_tarfile(tar_path: Path, extract_path: Path):\\n>>>     \"\"\"Extract a tar file to the specified path.\"\"\"\\n    logger.info(f\"\\\\n{\\'=\\'*20} Extracting Tar File {\\'=\\'*20}\")\\n    ' is_hardcoded=True construction_method=None",
        "path='Cannot extract. Tar file does not exist.' line_number=160 context='    \\n    if not check_file_exists(tar_path, \"Tar file to extract\"):\\n>>>         logger.error(\"Cannot extract. Tar file does not exist.\")\\n        return\\n    ' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=172 context='                size_mb = member.size / 1024 / 1024\\n                total_size += size_mb\\n>>>                 logger.info(f\"  {member.name} ({size_mb:.2f}MB)\")\\n            logger.info(f\"Total size in tar: {total_size:.2f}MB\")\\n            ' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=173 context='                total_size += size_mb\\n                logger.info(f\"  {member.name} ({size_mb:.2f}MB)\")\\n>>>             logger.info(f\"Total size in tar: {total_size:.2f}MB\")\\n            \\n            logger.info(f\"\\\\nExtracting to: {extract_path}\")' is_hardcoded=True construction_method=None",
        "path='Create a tar file from the contents of a directory.' line_number=186 context='\\ndef create_tarfile(output_tar_path: Path, source_dir: Path):\\n>>>     \"\"\"Create a tar file from the contents of a directory.\"\"\"\\n    logger.info(f\"\\\\n{\\'=\\'*20} Creating Tar File {\\'=\\'*20}\")\\n    logger.info(f\"Output tar: {output_tar_path}\")' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=204 context='                    total_size += size_mb\\n                    files_added += 1\\n>>>                     logger.info(f\"Adding to tar: {arcname} ({size_mb:.2f}MB)\")\\n                    tar.add(item, arcname=arcname)\\n        ' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=209 context='        logger.info(f\"\\\\nTar creation summary:\")\\n        logger.info(f\"  Files added: {files_added}\")\\n>>>         logger.info(f\"  Total uncompressed size: {total_size:.2f}MB\")\\n        \\n        if check_file_exists(output_tar_path, \"Created tar file\"):' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=213 context='        if check_file_exists(output_tar_path, \"Created tar file\"):\\n            compressed_size = output_tar_path.stat().st_size / 1024 / 1024\\n>>>             logger.info(f\"  Compressed tar size: {compressed_size:.2f}MB\")\\n            logger.info(f\"  Compression ratio: {compressed_size/total_size:.2%}\")\\n        ' is_hardcoded=True construction_method=None",
        "path='.2%' line_number=214 context='            compressed_size = output_tar_path.stat().st_size / 1024 / 1024\\n            logger.info(f\"  Compressed tar size: {compressed_size:.2f}MB\")\\n>>>             logger.info(f\"  Compression ratio: {compressed_size/total_size:.2%}\")\\n        \\n    except Exception as e:' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=224 context='    logger.info(f\"Python version: {sys.version}\")\\n    logger.info(f\"Working directory: {os.getcwd()}\")\\n>>>     logger.info(f\"Available disk space: {shutil.disk_usage(\\'/\\').free / (1024*1024*1024):.2f}GB\")\\n\\n    # Ensure working and output directories exist' is_hardcoded=True construction_method=None",
        "path='model.tar.gz' line_number=231 context='\\n    # Extract input model.tar.gz if it exists\\n>>>     input_model_tar = MODEL_PATH / \"model.tar.gz\"\\n    logger.info(\"\\\\nChecking for input model.tar.gz...\")\\n    ' is_hardcoded=True construction_method=None",
        "path='\\nChecking for input model.tar.gz...' line_number=232 context='    # Extract input model.tar.gz if it exists\\n    input_model_tar = MODEL_PATH / \"model.tar.gz\"\\n>>>     logger.info(\"\\\\nChecking for input model.tar.gz...\")\\n    \\n    if check_file_exists(input_model_tar, \"Input model.tar.gz\"):' is_hardcoded=True construction_method=None",
        "path='Input model.tar.gz' line_number=234 context='    logger.info(\"\\\\nChecking for input model.tar.gz...\")\\n    \\n>>>     if check_file_exists(input_model_tar, \"Input model.tar.gz\"):\\n        extract_tarfile(input_model_tar, WORKING_DIRECTORY)\\n    else:' is_hardcoded=True construction_method=None",
        "path='No model.tar.gz found. Copying all files from MODEL_PATH...' line_number=237 context='        extract_tarfile(input_model_tar, WORKING_DIRECTORY)\\n    else:\\n>>>         logger.info(\"No model.tar.gz found. Copying all files from MODEL_PATH...\")\\n        files_copied = 0\\n        total_size = 0' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=246 context='                    files_copied += 1\\n                    total_size += item.stat().st_size / 1024 / 1024\\n>>>         logger.info(f\"\\\\nCopied {files_copied} files, total size: {total_size:.2f}MB\")\\n\\n    # Copy inference scripts to WORKING_DIRECTORY/code' is_hardcoded=True construction_method=None",
        "path='model.tar.gz' line_number=252 context='\\n    # Create the output model.tar.gz\\n>>>     output_tar_file = OUTPUT_PATH / \"model.tar.gz\"\\n    create_tarfile(output_tar_file, WORKING_DIRECTORY)\\n' is_hardcoded=True construction_method=None"
      ],
      "env_var_accesses": [],
      "imports": [
        "module_name='shutil' import_alias=None line_number=1 is_from_import=False imported_items=[]",
        "module_name='tarfile' import_alias=None line_number=2 is_from_import=False imported_items=[]",
        "module_name='pathlib' import_alias=None line_number=3 is_from_import=True imported_items=['Path']",
        "module_name='logging' import_alias=None line_number=4 is_from_import=False imported_items=[]",
        "module_name='os' import_alias=None line_number=5 is_from_import=False imported_items=[]",
        "module_name='typing' import_alias=None line_number=6 is_from_import=True imported_items=['List', 'Dict', 'Optional']",
        "module_name='sys' import_alias=None line_number=7 is_from_import=False imported_items=[]"
      ],
      "argument_definitions": [],
      "file_operations": []
    },
    "contract": {
      "entry_point": "package.py",
      "inputs": {
        "model_input": {
          "path": "/opt/ml/processing/input/model"
        },
        "inference_scripts_input": {
          "path": "/opt/ml/processing/input/script"
        },
        "calibration_model": {
          "path": "/opt/ml/processing/input/calibration"
        }
      },
      "outputs": {
        "packaged_model": {
          "path": "/opt/ml/processing/output"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "\n    MIMS packaging script that:\n    1. Extracts model artifacts from input model directory or model.tar.gz\n    2. Includes calibration model if available\n    3. Copies inference scripts to code directory\n    4. Creates a packaged model.tar.gz file for deployment\n    4. Provides detailed logging of the packaging process\n    \n    Input Structure:\n    - /opt/ml/processing/input/model: Model artifacts (files or model.tar.gz)\n    - /opt/ml/processing/input/script: Inference scripts to include\n    - /opt/ml/processing/input/calibration: Optional calibration model artifacts\n    \n    Output Structure:\n    - /opt/ml/processing/output/model.tar.gz: Packaged model ready for deployment\n    ",
      "framework_requirements": {
        "python": ">=3.7"
      }
    }
  },
  "level2": {
    "passed": true,
    "issues": [],
    "contract": {
      "entry_point": "package.py",
      "inputs": {
        "model_input": {
          "path": "/opt/ml/processing/input/model"
        },
        "inference_scripts_input": {
          "path": "/opt/ml/processing/input/script"
        },
        "calibration_model": {
          "path": "/opt/ml/processing/input/calibration"
        }
      },
      "outputs": {
        "packaged_model": {
          "path": "/opt/ml/processing/output"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "\n    MIMS packaging script that:\n    1. Extracts model artifacts from input model directory or model.tar.gz\n    2. Includes calibration model if available\n    3. Copies inference scripts to code directory\n    4. Creates a packaged model.tar.gz file for deployment\n    4. Provides detailed logging of the packaging process\n    \n    Input Structure:\n    - /opt/ml/processing/input/model: Model artifacts (files or model.tar.gz)\n    - /opt/ml/processing/input/script: Inference scripts to include\n    - /opt/ml/processing/input/calibration: Optional calibration model artifacts\n    \n    Output Structure:\n    - /opt/ml/processing/output/model.tar.gz: Packaged model ready for deployment\n    ",
      "framework_requirements": {
        "python": ">=3.7"
      }
    },
    "specifications": {
      "package_spec": {
        "step_type": "Package",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "model_input",
            "dependency_type": "model_artifacts",
            "required": true,
            "compatible_sources": [
              "XGBoostTraining",
              "TrainingStep",
              "ModelStep"
            ],
            "data_type": "S3Uri",
            "description": "Trained model artifacts to be packaged"
          },
          {
            "logical_name": "inference_scripts_input",
            "dependency_type": "custom_property",
            "required": false,
            "compatible_sources": [
              "ScriptStep",
              "ProcessingStep"
            ],
            "data_type": "String",
            "description": "Inference scripts and code for model deployment (can be local directory path or S3 URI)"
          },
          {
            "logical_name": "calibration_model",
            "dependency_type": "processing_output",
            "required": false,
            "compatible_sources": [
              "ModelCalibration"
            ],
            "data_type": "S3Uri",
            "description": "Calibration model and artifacts for probability calibration (optional)"
          }
        ],
        "outputs": [
          {
            "logical_name": "packaged_model",
            "output_type": "model_artifacts",
            "property_path": "properties.ProcessingOutputConfig.Outputs['packaged_model'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Packaged model ready for deployment"
          }
        ]
      }
    },
    "unified_specification": {
      "primary_spec": {
        "step_type": "Package",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "model_input",
            "dependency_type": "model_artifacts",
            "required": true,
            "compatible_sources": [
              "XGBoostTraining",
              "TrainingStep",
              "ModelStep"
            ],
            "data_type": "S3Uri",
            "description": "Trained model artifacts to be packaged"
          },
          {
            "logical_name": "inference_scripts_input",
            "dependency_type": "custom_property",
            "required": false,
            "compatible_sources": [
              "ScriptStep",
              "ProcessingStep"
            ],
            "data_type": "String",
            "description": "Inference scripts and code for model deployment (can be local directory path or S3 URI)"
          },
          {
            "logical_name": "calibration_model",
            "dependency_type": "processing_output",
            "required": false,
            "compatible_sources": [
              "ModelCalibration"
            ],
            "data_type": "S3Uri",
            "description": "Calibration model and artifacts for probability calibration (optional)"
          }
        ],
        "outputs": [
          {
            "logical_name": "packaged_model",
            "output_type": "model_artifacts",
            "property_path": "properties.ProcessingOutputConfig.Outputs['packaged_model'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Packaged model ready for deployment"
          }
        ]
      },
      "variants": {
        "generic": {
          "step_type": "Package",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "model_input",
              "dependency_type": "model_artifacts",
              "required": true,
              "compatible_sources": [
                "XGBoostTraining",
                "TrainingStep",
                "ModelStep"
              ],
              "data_type": "S3Uri",
              "description": "Trained model artifacts to be packaged"
            },
            {
              "logical_name": "inference_scripts_input",
              "dependency_type": "custom_property",
              "required": false,
              "compatible_sources": [
                "ScriptStep",
                "ProcessingStep"
              ],
              "data_type": "String",
              "description": "Inference scripts and code for model deployment (can be local directory path or S3 URI)"
            },
            {
              "logical_name": "calibration_model",
              "dependency_type": "processing_output",
              "required": false,
              "compatible_sources": [
                "ModelCalibration"
              ],
              "data_type": "S3Uri",
              "description": "Calibration model and artifacts for probability calibration (optional)"
            }
          ],
          "outputs": [
            {
              "logical_name": "packaged_model",
              "output_type": "model_artifacts",
              "property_path": "properties.ProcessingOutputConfig.Outputs['packaged_model'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Packaged model ready for deployment"
            }
          ]
        }
      },
      "unified_dependencies": {
        "model_input": {
          "logical_name": "model_input",
          "dependency_type": "model_artifacts",
          "required": true,
          "compatible_sources": [
            "XGBoostTraining",
            "TrainingStep",
            "ModelStep"
          ],
          "data_type": "S3Uri",
          "description": "Trained model artifacts to be packaged"
        },
        "inference_scripts_input": {
          "logical_name": "inference_scripts_input",
          "dependency_type": "custom_property",
          "required": false,
          "compatible_sources": [
            "ScriptStep",
            "ProcessingStep"
          ],
          "data_type": "String",
          "description": "Inference scripts and code for model deployment (can be local directory path or S3 URI)"
        },
        "calibration_model": {
          "logical_name": "calibration_model",
          "dependency_type": "processing_output",
          "required": false,
          "compatible_sources": [
            "ModelCalibration"
          ],
          "data_type": "S3Uri",
          "description": "Calibration model and artifacts for probability calibration (optional)"
        }
      },
      "unified_outputs": {
        "packaged_model": {
          "logical_name": "packaged_model",
          "output_type": "model_artifacts",
          "property_path": "properties.ProcessingOutputConfig.Outputs['packaged_model'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Packaged model ready for deployment"
        }
      },
      "dependency_sources": {
        "model_input": [
          "generic"
        ],
        "inference_scripts_input": [
          "generic"
        ],
        "calibration_model": [
          "generic"
        ]
      },
      "output_sources": {
        "packaged_model": [
          "generic"
        ]
      },
      "variant_count": 1
    }
  },
  "level3": {
    "passed": true,
    "issues": [],
    "specification": {
      "step_type": "Package",
      "node_type": "internal",
      "dependencies": [
        {
          "logical_name": "model_input",
          "dependency_type": "model_artifacts",
          "required": true,
          "compatible_sources": [
            "XGBoostTraining",
            "TrainingStep",
            "ModelStep"
          ],
          "data_type": "S3Uri",
          "description": "Trained model artifacts to be packaged"
        },
        {
          "logical_name": "inference_scripts_input",
          "dependency_type": "custom_property",
          "required": false,
          "compatible_sources": [
            "ScriptStep",
            "ProcessingStep"
          ],
          "data_type": "String",
          "description": "Inference scripts and code for model deployment (can be local directory path or S3 URI)"
        },
        {
          "logical_name": "calibration_model",
          "dependency_type": "processing_output",
          "required": false,
          "compatible_sources": [
            "ModelCalibration"
          ],
          "data_type": "S3Uri",
          "description": "Calibration model and artifacts for probability calibration (optional)"
        }
      ],
      "outputs": [
        {
          "logical_name": "packaged_model",
          "output_type": "model_artifacts",
          "property_path": "properties.ProcessingOutputConfig.Outputs['packaged_model'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Packaged model ready for deployment"
        }
      ]
    }
  },
  "level4": {
    "passed": true,
    "issues": [],
    "builder_analysis": {
      "config_accesses": [],
      "validation_calls": [],
      "default_assignments": [],
      "class_definitions": [
        {
          "class_name": "PackageStepBuilder",
          "line_number": 27
        }
      ],
      "method_definitions": [
        {
          "method_name": "__init__",
          "line_number": 35
        },
        {
          "method_name": "validate_configuration",
          "line_number": 75
        },
        {
          "method_name": "_create_processor",
          "line_number": 104
        },
        {
          "method_name": "_get_environment_variables",
          "line_number": 130
        },
        {
          "method_name": "_get_inputs",
          "line_number": 160
        },
        {
          "method_name": "_get_outputs",
          "line_number": 260
        },
        {
          "method_name": "_get_job_arguments",
          "line_number": 315
        },
        {
          "method_name": "create_step",
          "line_number": 326
        }
      ]
    },
    "config_analysis": {
      "class_name": "PackageConfig",
      "fields": {
        "processing_entry_point": {
          "type": "<class 'str'>",
          "required": false
        }
      },
      "required_fields": [],
      "optional_fields": [
        "processing_entry_point"
      ],
      "default_values": {
        "aws_region": "<property>",
        "effective_instance_type": "<property>",
        "effective_source_dir": "<property>",
        "model_computed_fields": {},
        "model_config": {
          "arbitrary_types_allowed": true,
          "extra": "allow",
          "protected_namespaces": [],
          "validate_assignment": true
        },
        "model_extra": "<property>",
        "model_fields": {
          "author": "annotation=str required=True description='Author or owner of the pipeline.'",
          "bucket": "annotation=str required=True description='S3 bucket name for pipeline artifacts and data.'",
          "role": "annotation=str required=True description='IAM role for pipeline execution.'",
          "region": "annotation=str required=True description='Custom region code (NA, EU, FE) for internal logic.'",
          "service_name": "annotation=str required=True description='Service name for the pipeline.'",
          "pipeline_version": "annotation=str required=True description='Version string for the SageMaker Pipeline.'",
          "model_class": "annotation=str required=False default='xgboost' description='Model class (e.g., XGBoost, PyTorch).'",
          "current_date": "annotation=str required=False default_factory=<lambda> description='Current date, typically used for versioning or pathing.'",
          "framework_version": "annotation=str required=False default='2.1.0' description='Default framework version (e.g., PyTorch).'",
          "py_version": "annotation=str required=False default='py310' description='Default Python version.'",
          "source_dir": "annotation=Union[str, NoneType] required=False default=None description='Common source directory for scripts if applicable. Can be overridden by step configs.'",
          "processing_instance_count": "annotation=int required=False default=1 description='Instance count for processing jobs' metadata=[Ge(ge=1), Le(le=10)]",
          "processing_volume_size": "annotation=int required=False default=500 description='Volume size for processing jobs in GB' metadata=[Ge(ge=10), Le(le=1000)]",
          "processing_instance_type_large": "annotation=str required=False default='ml.m5.4xlarge' description='Large instance type for processing step.'",
          "processing_instance_type_small": "annotation=str required=False default='ml.m5.2xlarge' description='Small instance type for processing step.'",
          "use_large_processing_instance": "annotation=bool required=False default=False description='Set to True to use large instance type, False for small instance type.'",
          "processing_source_dir": "annotation=Union[str, NoneType] required=False default=None description='Source directory for processing scripts. Falls back to base source_dir if not provided.'",
          "processing_entry_point": "annotation=str required=False default='package.py' description='Entry point script for packaging.'",
          "processing_script_arguments": "annotation=Union[List[str], NoneType] required=False default=None description='Optional arguments for the processing script.'",
          "processing_framework_version": "annotation=str required=False default='1.2-1' description=\"Version of the scikit-learn framework to use in SageMaker Processing. Format: '<sklearn-version>-<build-number>'\""
        },
        "model_fields_set": "<property>",
        "pipeline_description": "<property>",
        "pipeline_name": "<property>",
        "pipeline_s3_loc": "<property>",
        "script_contract": "<property>",
        "script_path": "<property>"
      }
    }
  },
  "overall_status": "PASSING",
  "metadata": {
    "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/package.py",
    "contract_mapping": "package_contract",
    "validation_timestamp": "2025-08-11T21:34:37.205324",
    "validator_version": "1.0.0"
  }
}