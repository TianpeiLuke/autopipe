{
  "script_name": "risk_table_mapping",
  "level1": {
    "passed": true,
    "issues": [
      {
        "severity": "WARNING",
        "category": "logical_names",
        "message": "Script uses logical name not in contract: config",
        "details": {
          "logical_name": "config",
          "script": "risk_table_mapping"
        },
        "recommendation": "Add logical name config to contract or update script"
      },
      {
        "severity": "WARNING",
        "category": "logical_names",
        "message": "Script uses logical name not in contract: data",
        "details": {
          "logical_name": "data",
          "script": "risk_table_mapping"
        },
        "recommendation": "Add logical name data to contract or update script"
      },
      {
        "severity": "WARNING",
        "category": "arguments",
        "message": "Script defines argument not in contract: job_type",
        "details": {
          "argument": "job_type",
          "script": "risk_table_mapping"
        },
        "recommendation": "Add job_type to contract arguments or remove from script"
      },
      {
        "severity": "INFO",
        "category": "file_operations",
        "message": "Contract declares input not read by script: /opt/ml/processing/input/data",
        "details": {
          "path": "/opt/ml/processing/input/data",
          "operation": "read",
          "script": "risk_table_mapping"
        },
        "recommendation": "Either read /opt/ml/processing/input/data in script or remove from contract inputs"
      },
      {
        "severity": "INFO",
        "category": "file_operations",
        "message": "Contract declares input not read by script: /opt/ml/processing/input/risk_tables",
        "details": {
          "path": "/opt/ml/processing/input/risk_tables",
          "operation": "read",
          "script": "risk_table_mapping"
        },
        "recommendation": "Either read /opt/ml/processing/input/risk_tables in script or remove from contract inputs"
      },
      {
        "severity": "INFO",
        "category": "file_operations",
        "message": "Contract declares input not read by script: /opt/ml/processing/input/config",
        "details": {
          "path": "/opt/ml/processing/input/config",
          "operation": "read",
          "script": "risk_table_mapping"
        },
        "recommendation": "Either read /opt/ml/processing/input/config in script or remove from contract inputs"
      },
      {
        "severity": "WARNING",
        "category": "file_operations",
        "message": "Contract declares output not written by script: /opt/ml/processing/output",
        "details": {
          "path": "/opt/ml/processing/output",
          "operation": "write",
          "script": "risk_table_mapping"
        },
        "recommendation": "Either write to /opt/ml/processing/output in script or remove from contract outputs"
      }
    ],
    "script_analysis": {
      "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/risk_table_mapping.py",
      "path_references": [
        "path='\\nRisk Table Mapping Processing Script\\n\\nThis script creates and applies risk tables for categorical features based on \\ntarget variable correlation, and handles missing value imputation for numeric features.\\nIt supports both training mode (fit and transform) and inference mode (transform only).\\n' line_number=2 context='#!/usr/bin/env python\\n>>> \"\"\"\\nRisk Table Mapping Processing Script\\n' is_hardcoded=True construction_method=None",
        "path='bin_mapping.pkl' line_number=24 context='# These constants ensure the same filenames are used across all job types,\\n# facilitating proper connections between training and non-training steps\\n>>> RISK_TABLE_FILENAME = \"bin_mapping.pkl\"  # Used by downstream steps as input dependency\\nHYPERPARAMS_FILENAME = \"hyperparameters.json\"  # Expected by the script and generated by the builder\\n' is_hardcoded=True construction_method=None",
        "path='hyperparameters.json' line_number=25 context='# facilitating proper connections between training and non-training steps\\nRISK_TABLE_FILENAME = \"bin_mapping.pkl\"  # Used by downstream steps as input dependency\\n>>> HYPERPARAMS_FILENAME = \"hyperparameters.json\"  # Expected by the script and generated by the builder\\n\\n# Set up logging' is_hardcoded=True construction_method=None",
        "path='\\n    A class to create risk tables for categorical features.\\n    \\n    Risk tables map categorical values to numerical risk scores based on\\n    their correlation with the target variable.\\n    ' line_number=100 context='\\nclass OfflineBinning:\\n>>>     \"\"\"\\n    A class to create risk tables for categorical features.\\n    ' is_hardcoded=True construction_method=None",
        "path='Fits the risk tables based on the provided dataframe.' line_number=112 context='\\n    def fit(self, df: pd.DataFrame, smooth_factor: float = 0, count_threshold: int = 0):\\n>>>         \"\"\"Fits the risk tables based on the provided dataframe.\"\"\"\\n        # Drop any -1 or NaN target rows for fitting\\n        fit_df = df.loc[(df[self.target] != -1) & (~df[self.target].isnull())].copy()' is_hardcoded=True construction_method=None",
        "path='Helper to calculate the risk table for a single variable.' line_number=143 context='\\n    def _create_risk_table(self, df, variable, default_risk, samples, count_threshold):\\n>>>         \"\"\"Helper to calculate the risk table for a single variable.\"\"\"\\n        cross_tab = pd.crosstab(df[variable], df[self.target].astype(object), margins=True, margins_name=\"_count_\", dropna=False)\\n        cross_tab[\"risk\"] = cross_tab.apply(lambda x: x.get(1, 0.0) / (x.get(1, 0) + x.get(0, 0)), axis=1)' is_hardcoded=True construction_method=None",
        "path='Transforms the dataframe using the fitted risk tables.' line_number=154 context='\\n    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\\n>>>         \"\"\"Transforms the dataframe using the fitted risk tables.\"\"\"\\n        df_transformed = df.copy()\\n        for var, risk_table_info in self.risk_tables.items():' is_hardcoded=True construction_method=None",
        "path='Loads pre-existing risk tables.' line_number=164 context='\\n    def load_risk_tables(self, risk_tables):\\n>>>         \"\"\"Loads pre-existing risk tables.\"\"\"\\n        self.risk_tables = risk_tables\\n        logger.info(f\"Loaded {len(risk_tables)} risk tables\")' is_hardcoded=True construction_method=None",
        "path='train_processed_data.csv' line_number=184 context='    if job_type == \"training\":\\n        # For training, we expect data in train/test/val subdirectories\\n>>>         train_df = pd.read_csv(input_path / \"train\" / \"train_processed_data.csv\")\\n        test_df = pd.read_csv(input_path / \"test\" / \"test_processed_data.csv\")\\n        val_df = pd.read_csv(input_path / \"val\" / \"val_processed_data.csv\")' is_hardcoded=True construction_method=None",
        "path='test_processed_data.csv' line_number=185 context='        # For training, we expect data in train/test/val subdirectories\\n        train_df = pd.read_csv(input_path / \"train\" / \"train_processed_data.csv\")\\n>>>         test_df = pd.read_csv(input_path / \"test\" / \"test_processed_data.csv\")\\n        val_df = pd.read_csv(input_path / \"val\" / \"val_processed_data.csv\")\\n        logger.info(f\"Loaded training data splits: train={train_df.shape}, test={test_df.shape}, val={val_df.shape}\")' is_hardcoded=True construction_method=None",
        "path='val_processed_data.csv' line_number=186 context='        train_df = pd.read_csv(input_path / \"train\" / \"train_processed_data.csv\")\\n        test_df = pd.read_csv(input_path / \"test\" / \"test_processed_data.csv\")\\n>>>         val_df = pd.read_csv(input_path / \"val\" / \"val_processed_data.csv\")\\n        logger.info(f\"Loaded training data splits: train={train_df.shape}, test={test_df.shape}, val={val_df.shape}\")\\n        return {\"train\": train_df, \"test\": test_df, \"val\": val_df}' is_hardcoded=True construction_method=None",
        "path='_processed_data.csv' line_number=191 context='    else:\\n        # For other job types, we expect data in a single directory named after job_type\\n>>>         df = pd.read_csv(input_path / job_type / f\"{job_type}_processed_data.csv\")\\n        logger.info(f\"Loaded {job_type} data: {df.shape}\")\\n        return {job_type: df}' is_hardcoded=True construction_method=None",
        "path='_processed_data.csv' line_number=209 context='        split_output_dir.mkdir(exist_ok=True, parents=True)\\n        \\n>>>         output_file = split_output_dir / f\"{split_name}_processed_data.csv\"\\n        df.to_csv(output_file, index=False)\\n        logger.info(f\"Saved {split_name} data to {output_file}, shape: {df.shape}\")' is_hardcoded=True construction_method=None",
        "path='No valid categorical fields found for risk mapping. Using original data.' line_number=244 context='        \\n        if not valid_cat_fields:\\n>>>             logger.warning(\"No valid categorical fields found for risk mapping. Using original data.\")\\n            transformed_data = data_dict\\n            ' is_hardcoded=True construction_method=None",
        "path='Risk-table mapping complete.' line_number=403 context='    save_artifacts(binner, hyperparams, output_path)\\n\\n>>>     logger.info(\"Risk-table mapping complete.\")\\n    return transformed_data, binner\\n' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/data' line_number=416 context='        \\n        # Define standard SageMaker paths based on contract\\n>>>         input_dir = \"/opt/ml/processing/input/data\"\\n        config_dir = \"/opt/ml/processing/input/config\"\\n        output_dir = \"/opt/ml/processing/output\"' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/config' line_number=417 context='        # Define standard SageMaker paths based on contract\\n        input_dir = \"/opt/ml/processing/input/data\"\\n>>>         config_dir = \"/opt/ml/processing/input/config\"\\n        output_dir = \"/opt/ml/processing/output\"\\n        risk_table_input_dir = \"/opt/ml/processing/input/risk_tables\" if args.job_type != \"training\" else None' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output' line_number=418 context='        input_dir = \"/opt/ml/processing/input/data\"\\n        config_dir = \"/opt/ml/processing/input/config\"\\n>>>         output_dir = \"/opt/ml/processing/output\"\\n        risk_table_input_dir = \"/opt/ml/processing/input/risk_tables\" if args.job_type != \"training\" else None\\n' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/risk_tables' line_number=419 context='        config_dir = \"/opt/ml/processing/input/config\"\\n        output_dir = \"/opt/ml/processing/output\"\\n>>>         risk_table_input_dir = \"/opt/ml/processing/input/risk_tables\" if args.job_type != \"training\" else None\\n\\n        # Log input/output paths for clarity' is_hardcoded=True construction_method=None"
      ],
      "env_var_accesses": [],
      "imports": [
        "module_name='argparse' import_alias=None line_number=10 is_from_import=False imported_items=[]",
        "module_name='os' import_alias=None line_number=11 is_from_import=False imported_items=[]",
        "module_name='sys' import_alias=None line_number=12 is_from_import=False imported_items=[]",
        "module_name='pandas' import_alias='pd' line_number=13 is_from_import=False imported_items=[]",
        "module_name='numpy' import_alias='np' line_number=14 is_from_import=False imported_items=[]",
        "module_name='json' import_alias=None line_number=15 is_from_import=False imported_items=[]",
        "module_name='pickle' import_alias='pkl' line_number=16 is_from_import=False imported_items=[]",
        "module_name='pathlib' import_alias=None line_number=17 is_from_import=True imported_items=['Path']",
        "module_name='sklearn.impute' import_alias=None line_number=18 is_from_import=True imported_items=['SimpleImputer']",
        "module_name='logging' import_alias=None line_number=19 is_from_import=False imported_items=[]",
        "module_name='traceback' import_alias=None line_number=460 is_from_import=False imported_items=[]"
      ],
      "argument_definitions": [
        "argument_name='job_type' line_number=410 is_required=True has_default=False default_value=None argument_type='str' choices=['training', 'validation', 'testing', 'calibration']"
      ],
      "file_operations": []
    },
    "contract": {
      "entry_point": "risk_table_mapping.py",
      "inputs": {
        "data_input": {
          "path": "/opt/ml/processing/input/data"
        },
        "hyperparameters_s3_uri": {
          "path": "/opt/ml/processing/input/config"
        },
        "risk_tables": {
          "path": "/opt/ml/processing/input/risk_tables"
        }
      },
      "outputs": {
        "processed_data": {
          "path": "/opt/ml/processing/output"
        },
        "risk_tables": {
          "path": "/opt/ml/processing/output"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "\n    Risk table mapping script that:\n    1. Creates risk tables for categorical features based on target variable correlation\n    2. Handles missing value imputation for numeric features\n    3. Supports both training mode (fit and transform) and inference mode (transform only)\n    4. Applies smoothing and count thresholds for robust risk estimation\n    5. Saves fitted artifacts for reuse in inference\n    \n    Input Structure:\n    - /opt/ml/processing/input/data: Data files from tabular preprocessing\n      - Training mode: train/, test/, val/ subdirectories with processed data\n      - Other modes: job_type/ subdirectory with processed data\n    - /opt/ml/processing/input/config: Configuration files\n      - config.json: Model configuration including category risk parameters\n      - metadata.csv: Variable metadata with types and imputation strategies\n      - job_type: Configuration parameter specifying job type (training, validation, testing, calibration)\n    - /opt/ml/processing/input/risk_tables: Pre-trained risk tables (for non-training modes)\n      - bin_mapping.pkl: Risk table mappings for categorical features\n      - missing_value_imputation.pkl: Imputation values for numeric features\n    \n    Output Structure:\n    - /opt/ml/processing/output/{split}/{split}_processed_data.csv: Transformed data by split\n    - /opt/ml/processing/output/bin_mapping.pkl: Risk table mappings for categorical features\n    - /opt/ml/processing/output/missing_value_imputation.pkl: Imputation values for numeric features\n    - /opt/ml/processing/output/config.pkl: Serialized configuration with metadata\n    \n    Job Types (from config):\n    - training: Fits risk tables on training data, transforms all splits\n    - validation/testing/calibration: Uses pre-trained risk tables, transforms single split\n    \n    Training Mode:\n    - Fits risk tables on training data\n    - Transforms train/test/val splits\n    - Saves risk tables and imputation models\n    \n    Non-Training Modes:\n    - Loads pre-trained risk tables and imputation models\n    - Transforms data using loaded artifacts\n    - Maintains the same output structure as training mode\n    ",
      "framework_requirements": {
        "pandas": ">=1.3.0",
        "numpy": ">=1.21.0",
        "scikit-learn": ">=1.0.0"
      }
    }
  },
  "level2": {
    "passed": false,
    "issues": [
      {
        "severity": "ERROR",
        "category": "logical_names",
        "message": "Contract input risk_tables not declared as specification dependency",
        "details": {
          "logical_name": "risk_tables",
          "contract": "risk_table_mapping"
        },
        "recommendation": "Add risk_tables to specification dependencies"
      }
    ],
    "contract": {
      "entry_point": "risk_table_mapping.py",
      "inputs": {
        "data_input": "/opt/ml/processing/input/data",
        "hyperparameters_s3_uri": "/opt/ml/processing/input/config",
        "risk_tables": "/opt/ml/processing/input/risk_tables"
      },
      "outputs": {
        "processed_data": "/opt/ml/processing/output",
        "risk_tables": "/opt/ml/processing/output"
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "framework_requirements": {
        "pandas": ">=1.3.0",
        "numpy": ">=1.21.0",
        "scikit-learn": ">=1.0.0"
      },
      "description": "\n    Risk table mapping script that:\n    1. Creates risk tables for categorical features based on target variable correlation\n    2. Handles missing value imputation for numeric features\n    3. Supports both training mode (fit and transform) and inference mode (transform only)\n    4. Applies smoothing and count thresholds for robust risk estimation\n    5. Saves fitted artifacts for reuse in inference\n    \n    Input Structure:\n    - /opt/ml/processing/input/data: Data files from tabular preprocessing\n      - Training mode: train/, test/, val/ subdirectories with processed data\n      - Other modes: job_type/ subdirectory with processed data\n    - /opt/ml/processing/input/config: Configuration files\n      - config.json: Model configuration including category risk parameters\n      - metadata.csv: Variable metadata with types and imputation strategies\n      - job_type: Configuration parameter specifying job type (training, validation, testing, calibration)\n    - /opt/ml/processing/input/risk_tables: Pre-trained risk tables (for non-training modes)\n      - bin_mapping.pkl: Risk table mappings for categorical features\n      - missing_value_imputation.pkl: Imputation values for numeric features\n    \n    Output Structure:\n    - /opt/ml/processing/output/{split}/{split}_processed_data.csv: Transformed data by split\n    - /opt/ml/processing/output/bin_mapping.pkl: Risk table mappings for categorical features\n    - /opt/ml/processing/output/missing_value_imputation.pkl: Imputation values for numeric features\n    - /opt/ml/processing/output/config.pkl: Serialized configuration with metadata\n    \n    Job Types (from config):\n    - training: Fits risk tables on training data, transforms all splits\n    - validation/testing/calibration: Uses pre-trained risk tables, transforms single split\n    \n    Training Mode:\n    - Fits risk tables on training data\n    - Transforms train/test/val splits\n    - Saves risk tables and imputation models\n    \n    Non-Training Modes:\n    - Loads pre-trained risk tables and imputation models\n    - Transforms data using loaded artifacts\n    - Maintains the same output structure as training mode\n    "
    },
    "specifications": {
      "testing": {
        "step_type": "RiskTableMapping_Testing",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "data_input",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "TabularPreprocessing",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "Preprocessed testing data from tabular preprocessing step"
          },
          {
            "logical_name": "hyperparameters_s3_uri",
            "dependency_type": "hyperparameters",
            "required": false,
            "compatible_sources": [
              "ModelTraining",
              "ConfigurationStep",
              "ProcessingStep",
              "FeatureEngineering",
              "DataPrep",
              "HyperparameterPrep",
              "DataQuality"
            ],
            "data_type": "S3Uri",
            "description": "Optional external hyperparameters configuration file (will be overridden by internal generation)"
          },
          {
            "logical_name": "risk_tables",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "RiskTableMapping_Training"
            ],
            "data_type": "S3Uri",
            "description": "Risk tables and imputation models from training step"
          }
        ],
        "outputs": [
          {
            "logical_name": "processed_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Processed testing data with risk table mappings applied"
          },
          {
            "logical_name": "risk_tables",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['risk_tables'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Risk tables and imputation models (passthrough from training)"
          }
        ]
      },
      "calibration": {
        "step_type": "RiskTableMapping_Calibration",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "data_input",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "TabularPreprocessing",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "Preprocessed calibration data from tabular preprocessing step"
          },
          {
            "logical_name": "hyperparameters_s3_uri",
            "dependency_type": "hyperparameters",
            "required": false,
            "compatible_sources": [
              "ModelTraining",
              "ConfigurationStep",
              "ProcessingStep",
              "FeatureEngineering",
              "DataPrep",
              "HyperparameterPrep",
              "DataQuality"
            ],
            "data_type": "S3Uri",
            "description": "Optional external hyperparameters configuration file (will be overridden by internal generation)"
          },
          {
            "logical_name": "risk_tables",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "RiskTableMapping_Training"
            ],
            "data_type": "S3Uri",
            "description": "Risk tables and imputation models from training step"
          }
        ],
        "outputs": [
          {
            "logical_name": "processed_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Processed calibration data with risk table mappings applied"
          },
          {
            "logical_name": "risk_tables",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['risk_tables'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Risk tables and imputation models (passthrough from training)"
          }
        ]
      },
      "validation": {
        "step_type": "RiskTableMapping_Validation",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "data_input",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "TabularPreprocessing",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "Preprocessed validation data from tabular preprocessing step"
          },
          {
            "logical_name": "hyperparameters_s3_uri",
            "dependency_type": "hyperparameters",
            "required": false,
            "compatible_sources": [
              "ModelTraining",
              "ConfigurationStep",
              "ProcessingStep",
              "FeatureEngineering",
              "DataPrep",
              "HyperparameterPrep",
              "DataQuality"
            ],
            "data_type": "S3Uri",
            "description": "Optional external hyperparameters configuration file (will be overridden by internal generation)"
          },
          {
            "logical_name": "risk_tables",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "RiskTableMapping_Training"
            ],
            "data_type": "S3Uri",
            "description": "Risk tables and imputation models from training step"
          }
        ],
        "outputs": [
          {
            "logical_name": "processed_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Processed validation data with risk table mappings applied"
          },
          {
            "logical_name": "risk_tables",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['risk_tables'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Risk tables and imputation models (passthrough from training)"
          }
        ]
      },
      "training": {
        "step_type": "RiskTableMapping_Training",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "data_input",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "TabularPreprocessing",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "Preprocessed training data from tabular preprocessing step"
          },
          {
            "logical_name": "hyperparameters_s3_uri",
            "dependency_type": "hyperparameters",
            "required": false,
            "compatible_sources": [
              "ModelTraining",
              "ConfigurationStep",
              "ProcessingStep",
              "FeatureEngineering",
              "DataPrep",
              "HyperparameterPrep",
              "DataQuality"
            ],
            "data_type": "S3Uri",
            "description": "Optional external hyperparameters configuration file (will be overridden by internal generation)"
          }
        ],
        "outputs": [
          {
            "logical_name": "processed_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['processed_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Processed data with risk table mappings applied"
          },
          {
            "logical_name": "risk_tables",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['risk_tables'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Risk tables and imputation models for categorical features"
          }
        ]
      }
    }
  },
  "level3": {
    "passed": false,
    "issues": [
      {
        "severity": "CRITICAL",
        "category": "missing_file",
        "message": "Specification file not found: /Users/tianpeixie/github_workspace/cursus/src/cursus/steps/specs/risk_table_mapping_spec.json",
        "recommendation": "Create the specification file risk_table_mapping_spec.json"
      }
    ]
  },
  "level4": {
    "passed": false,
    "issues": [
      {
        "severity": "ERROR",
        "category": "missing_configuration",
        "message": "Configuration file not found: src/cursus/steps/configs/config_risk_table_mapping_step.py",
        "recommendation": "Create configuration file config_risk_table_mapping_step.py"
      }
    ]
  },
  "overall_status": "FAILING",
  "metadata": {
    "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/risk_table_mapping.py",
    "contract_mapping": "risk_table_mapping_contract",
    "validation_timestamp": "2025-08-08T23:45:56.232548",
    "validator_version": "1.0.0"
  }
}