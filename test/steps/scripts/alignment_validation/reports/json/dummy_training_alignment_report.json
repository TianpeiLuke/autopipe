{
  "script_name": "dummy_training",
  "level1": {
    "passed": true,
    "issues": [],
    "script_analysis": {
      "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/dummy_training.py",
      "path_references": [
        "path='\\nDummyTraining Processing Script\\n\\nThis script validates, unpacks a pretrained model.tar.gz file, adds a hyperparameters.json file \\ninside it, then repacks it and outputs to the destination. It serves as a dummy training step \\nthat skips actual training and integrates with downstream MIMS packaging and payload steps.\\n' line_number=2 context='#!/usr/bin/env python\\n>>> \"\"\"\\nDummyTraining Processing Script\\n' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/model/model.tar.gz' line_number=33 context='# - hyperparameters_s3_uri: \"/opt/ml/processing/input/config/hyperparameters.json\"\\n# - model_input: \"/opt/ml/processing/output/model\" (aligns with packaging step dependency)\\n>>> MODEL_INPUT_PATH = \"/opt/ml/processing/input/model/model.tar.gz\"\\nHYPERPARAMS_INPUT_PATH = \"/opt/ml/processing/input/config/hyperparameters.json\"\\nMODEL_OUTPUT_DIR = \"/opt/ml/processing/output/model\"' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/config/hyperparameters.json' line_number=34 context='# - model_input: \"/opt/ml/processing/output/model\" (aligns with packaging step dependency)\\nMODEL_INPUT_PATH = \"/opt/ml/processing/input/model/model.tar.gz\"\\n>>> HYPERPARAMS_INPUT_PATH = \"/opt/ml/processing/input/config/hyperparameters.json\"\\nMODEL_OUTPUT_DIR = \"/opt/ml/processing/output/model\"\\n' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/model' line_number=35 context='MODEL_INPUT_PATH = \"/opt/ml/processing/input/model/model.tar.gz\"\\nHYPERPARAMS_INPUT_PATH = \"/opt/ml/processing/input/config/hyperparameters.json\"\\n>>> MODEL_OUTPUT_DIR = \"/opt/ml/processing/output/model\"\\n\\ndef validate_model(input_path: Path) -> bool:' is_hardcoded=True construction_method=None",
        "path='.tar.gz' line_number=54 context='    \\n    # Check file extension\\n>>>     if not input_path.suffix == \\'.tar.gz\\' and not str(input_path).endswith(\\'.tar.gz\\'):\\n        raise ValueError(f\"Expected a .tar.gz file, but got: {input_path} (ERROR_CODE: INVALID_FORMAT)\")\\n    ' is_hardcoded=True construction_method=None",
        "path='.tar.gz' line_number=54 context='    \\n    # Check file extension\\n>>>     if not input_path.suffix == \\'.tar.gz\\' and not str(input_path).endswith(\\'.tar.gz\\'):\\n        raise ValueError(f\"Expected a .tar.gz file, but got: {input_path} (ERROR_CODE: INVALID_FORMAT)\")\\n    ' is_hardcoded=True construction_method=None",
        "path='Ensure a directory exists, creating it if necessary.' line_number=70 context='\\ndef ensure_directory(directory: Path):\\n>>>     \"\"\"Ensure a directory exists, creating it if necessary.\"\"\"\\n    try:\\n        directory.mkdir(parents=True, exist_ok=True)' is_hardcoded=True construction_method=None",
        "path='Extract a tar file to the specified path.' line_number=80 context='\\ndef extract_tarfile(tar_path: Path, extract_path: Path):\\n>>>     \"\"\"Extract a tar file to the specified path.\"\"\"\\n    logger.info(f\"Extracting tar file: {tar_path} to {extract_path}\")\\n    ' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=95 context='                size_mb = member.size / 1024 / 1024\\n                total_size += size_mb\\n>>>                 logger.info(f\"  {member.name} ({size_mb:.2f}MB)\")\\n            logger.info(f\"Total size in tar: {total_size:.2f}MB\")\\n            ' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=96 context='                total_size += size_mb\\n                logger.info(f\"  {member.name} ({size_mb:.2f}MB)\")\\n>>>             logger.info(f\"Total size in tar: {total_size:.2f}MB\")\\n            \\n            logger.info(f\"Extracting to: {extract_path}\")' is_hardcoded=True construction_method=None",
        "path='Create a tar file from the contents of a directory.' line_number=108 context='\\ndef create_tarfile(output_tar_path: Path, source_dir: Path):\\n>>>     \"\"\"Create a tar file from the contents of a directory.\"\"\"\\n    logger.info(f\"Creating tar file: {output_tar_path} from {source_dir}\")\\n    ' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=124 context='                    total_size += size_mb\\n                    files_added += 1\\n>>>                     logger.info(f\"Adding to tar: {arcname} ({size_mb:.2f}MB)\")\\n                    tar.add(item, arcname=arcname)\\n        ' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=129 context='        logger.info(f\"Tar creation summary:\")\\n        logger.info(f\"  Files added: {files_added}\")\\n>>>         logger.info(f\"  Total uncompressed size: {total_size:.2f}MB\")\\n        \\n        if output_tar_path.exists():' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=133 context='        if output_tar_path.exists():\\n            compressed_size = output_tar_path.stat().st_size / 1024 / 1024\\n>>>             logger.info(f\"  Compressed tar size: {compressed_size:.2f}MB\")\\n            logger.info(f\"  Compression ratio: {compressed_size/total_size:.2%}\")\\n        ' is_hardcoded=True construction_method=None",
        "path='.2%' line_number=134 context='            compressed_size = output_tar_path.stat().st_size / 1024 / 1024\\n            logger.info(f\"  Compressed tar size: {compressed_size:.2f}MB\")\\n>>>             logger.info(f\"  Compression ratio: {compressed_size/total_size:.2%}\")\\n        \\n    except Exception as e:' is_hardcoded=True construction_method=None",
        "path='Copy a file and ensure the destination directory exists.' line_number=141 context='\\ndef copy_file(src: Path, dst: Path):\\n>>>     \"\"\"Copy a file and ensure the destination directory exists.\"\"\"\\n    logger.info(f\"Copying file: {src} to {dst}\")\\n    ' is_hardcoded=True construction_method=None",
        "path='hyperparameters.json' line_number=193 context='        \\n        # Copy hyperparameters.json to the working directory\\n>>>         hyperparams_dest = working_dir / \"hyperparameters.json\"\\n        copy_file(hyperparams_path, hyperparams_dest)\\n        ' is_hardcoded=True construction_method=None",
        "path='model.tar.gz' line_number=200 context='        \\n        # Create the output model.tar.gz\\n>>>         output_path = output_dir / \"model.tar.gz\"\\n        create_tarfile(output_path, working_dir)\\n        ' is_hardcoded=True construction_method=None",
        "path='No hyperparameters file found. Falling back to simple copy mode.' line_number=234 context='        else:\\n            # For backward compatibility: just validate and copy the model\\n>>>             logger.info(\"No hyperparameters file found. Falling back to simple copy mode.\")\\n            validate_model(model_path)\\n            output_path = output_dir / \"model.tar.gz\"' is_hardcoded=True construction_method=None",
        "path='model.tar.gz' line_number=236 context='            logger.info(\"No hyperparameters file found. Falling back to simple copy mode.\")\\n            validate_model(model_path)\\n>>>             output_path = output_dir / \"model.tar.gz\"\\n            ensure_directory(output_dir)\\n            copy_file(model_path, output_path)' is_hardcoded=True construction_method=None"
      ],
      "env_var_accesses": [],
      "imports": [
        "module_name='argparse' import_alias=None line_number=10 is_from_import=False imported_items=[]",
        "module_name='json' import_alias=None line_number=11 is_from_import=False imported_items=[]",
        "module_name='logging' import_alias=None line_number=12 is_from_import=False imported_items=[]",
        "module_name='os' import_alias=None line_number=13 is_from_import=False imported_items=[]",
        "module_name='shutil' import_alias=None line_number=14 is_from_import=False imported_items=[]",
        "module_name='sys' import_alias=None line_number=15 is_from_import=False imported_items=[]",
        "module_name='tarfile' import_alias=None line_number=16 is_from_import=False imported_items=[]",
        "module_name='tempfile' import_alias=None line_number=17 is_from_import=False imported_items=[]",
        "module_name='pathlib' import_alias=None line_number=18 is_from_import=True imported_items=['Path']",
        "module_name='traceback' import_alias=None line_number=254 is_from_import=False imported_items=[]"
      ],
      "argument_definitions": [],
      "file_operations": []
    },
    "contract": {
      "entry_point": "dummy_training.py",
      "inputs": {
        "pretrained_model_path": {
          "path": "/opt/ml/processing/input/model/model.tar.gz"
        },
        "hyperparameters_s3_uri": {
          "path": "/opt/ml/processing/input/config/hyperparameters.json"
        }
      },
      "outputs": {
        "model_input": {
          "path": "/opt/ml/processing/output/model"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "Contract for dummy training step that processes a pretrained model.tar.gz by unpacking it, adding a hyperparameters.json file inside, and repacking it for downstream steps",
      "framework_requirements": {
        "boto3": ">=1.26.0",
        "pathlib": ">=1.0.0"
      }
    }
  },
  "level2": {
    "passed": true,
    "issues": [],
    "contract": {
      "entry_point": "dummy_training.py",
      "inputs": {
        "pretrained_model_path": {
          "path": "/opt/ml/processing/input/model/model.tar.gz"
        },
        "hyperparameters_s3_uri": {
          "path": "/opt/ml/processing/input/config/hyperparameters.json"
        }
      },
      "outputs": {
        "model_input": {
          "path": "/opt/ml/processing/output/model"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "Contract for dummy training step that processes a pretrained model.tar.gz by unpacking it, adding a hyperparameters.json file inside, and repacking it for downstream steps",
      "framework_requirements": {
        "boto3": ">=1.26.0",
        "pathlib": ">=1.0.0"
      }
    },
    "specifications": {
      "dummy_training_spec": {
        "step_type": "DummyTraining",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "pretrained_model_path",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "XGBoostTraining",
              "ProcessingStep",
              "PytorchTraining",
              "TabularPreprocessing"
            ],
            "data_type": "S3Uri",
            "description": "Path to pretrained model.tar.gz file"
          },
          {
            "logical_name": "hyperparameters_s3_uri",
            "dependency_type": "hyperparameters",
            "required": true,
            "compatible_sources": [
              "ProcessingStep",
              "HyperparameterPrep"
            ],
            "data_type": "S3Uri",
            "description": "Hyperparameters configuration file for inclusion in the model package"
          }
        ],
        "outputs": [
          {
            "logical_name": "model_input",
            "output_type": "model_artifacts",
            "property_path": "properties.ProcessingOutputConfig.Outputs['model_input'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "S3 path to model artifacts with integrated hyperparameters"
          }
        ]
      }
    },
    "unified_specification": {
      "primary_spec": {
        "step_type": "DummyTraining",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "pretrained_model_path",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "XGBoostTraining",
              "ProcessingStep",
              "PytorchTraining",
              "TabularPreprocessing"
            ],
            "data_type": "S3Uri",
            "description": "Path to pretrained model.tar.gz file"
          },
          {
            "logical_name": "hyperparameters_s3_uri",
            "dependency_type": "hyperparameters",
            "required": true,
            "compatible_sources": [
              "ProcessingStep",
              "HyperparameterPrep"
            ],
            "data_type": "S3Uri",
            "description": "Hyperparameters configuration file for inclusion in the model package"
          }
        ],
        "outputs": [
          {
            "logical_name": "model_input",
            "output_type": "model_artifacts",
            "property_path": "properties.ProcessingOutputConfig.Outputs['model_input'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "S3 path to model artifacts with integrated hyperparameters"
          }
        ]
      },
      "variants": {
        "training": {
          "step_type": "DummyTraining",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "pretrained_model_path",
              "dependency_type": "processing_output",
              "required": true,
              "compatible_sources": [
                "XGBoostTraining",
                "ProcessingStep",
                "PytorchTraining",
                "TabularPreprocessing"
              ],
              "data_type": "S3Uri",
              "description": "Path to pretrained model.tar.gz file"
            },
            {
              "logical_name": "hyperparameters_s3_uri",
              "dependency_type": "hyperparameters",
              "required": true,
              "compatible_sources": [
                "ProcessingStep",
                "HyperparameterPrep"
              ],
              "data_type": "S3Uri",
              "description": "Hyperparameters configuration file for inclusion in the model package"
            }
          ],
          "outputs": [
            {
              "logical_name": "model_input",
              "output_type": "model_artifacts",
              "property_path": "properties.ProcessingOutputConfig.Outputs['model_input'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "S3 path to model artifacts with integrated hyperparameters"
            }
          ]
        }
      },
      "unified_dependencies": {
        "pretrained_model_path": {
          "logical_name": "pretrained_model_path",
          "dependency_type": "processing_output",
          "required": true,
          "compatible_sources": [
            "XGBoostTraining",
            "ProcessingStep",
            "PytorchTraining",
            "TabularPreprocessing"
          ],
          "data_type": "S3Uri",
          "description": "Path to pretrained model.tar.gz file"
        },
        "hyperparameters_s3_uri": {
          "logical_name": "hyperparameters_s3_uri",
          "dependency_type": "hyperparameters",
          "required": true,
          "compatible_sources": [
            "ProcessingStep",
            "HyperparameterPrep"
          ],
          "data_type": "S3Uri",
          "description": "Hyperparameters configuration file for inclusion in the model package"
        }
      },
      "unified_outputs": {
        "model_input": {
          "logical_name": "model_input",
          "output_type": "model_artifacts",
          "property_path": "properties.ProcessingOutputConfig.Outputs['model_input'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "S3 path to model artifacts with integrated hyperparameters"
        }
      },
      "dependency_sources": {
        "pretrained_model_path": [
          "training"
        ],
        "hyperparameters_s3_uri": [
          "training"
        ]
      },
      "output_sources": {
        "model_input": [
          "training"
        ]
      },
      "variant_count": 1
    }
  },
  "level3": {
    "passed": false,
    "issues": [
      {
        "severity": "ERROR",
        "category": "dependency_resolution",
        "message": "Cannot resolve required dependency: pretrained_model_path",
        "details": {
          "logical_name": "pretrained_model_path",
          "specification": "dummy_training",
          "compatible_sources": [
            "XGBoostTraining",
            "ProcessingStep",
            "PytorchTraining",
            "TabularPreprocessing"
          ],
          "dependency_type": "processing_output",
          "available_steps": [
            "DataLoading",
            "Preprocessing",
            "CurrencyConversion",
            "ModelEval",
            "XgboostModel",
            "Registration",
            "RiskTableMapping",
            "BatchTransform",
            "Dummy",
            "Model",
            "Payload",
            "Xgboost",
            "Pytorch",
            "Packaging",
            "PytorchModel"
          ]
        },
        "recommendation": "Ensure a step exists that produces output pretrained_model_path"
      },
      {
        "severity": "ERROR",
        "category": "dependency_resolution",
        "message": "Cannot resolve required dependency: hyperparameters_s3_uri",
        "details": {
          "logical_name": "hyperparameters_s3_uri",
          "specification": "dummy_training",
          "compatible_sources": [
            "ProcessingStep",
            "HyperparameterPrep"
          ],
          "dependency_type": "hyperparameters",
          "available_steps": [
            "DataLoading",
            "Preprocessing",
            "CurrencyConversion",
            "ModelEval",
            "XgboostModel",
            "Registration",
            "RiskTableMapping",
            "BatchTransform",
            "Dummy",
            "Model",
            "Payload",
            "Xgboost",
            "Pytorch",
            "Packaging",
            "PytorchModel"
          ]
        },
        "recommendation": "Ensure a step exists that produces output hyperparameters_s3_uri"
      }
    ],
    "specification": {
      "step_type": "DummyTraining",
      "node_type": "internal",
      "dependencies": [
        {
          "logical_name": "pretrained_model_path",
          "dependency_type": "processing_output",
          "required": true,
          "compatible_sources": [
            "XGBoostTraining",
            "ProcessingStep",
            "PytorchTraining",
            "TabularPreprocessing"
          ],
          "data_type": "S3Uri",
          "description": "Path to pretrained model.tar.gz file"
        },
        {
          "logical_name": "hyperparameters_s3_uri",
          "dependency_type": "hyperparameters",
          "required": true,
          "compatible_sources": [
            "ProcessingStep",
            "HyperparameterPrep"
          ],
          "data_type": "S3Uri",
          "description": "Hyperparameters configuration file for inclusion in the model package"
        }
      ],
      "outputs": [
        {
          "logical_name": "model_input",
          "output_type": "model_artifacts",
          "property_path": "properties.ProcessingOutputConfig.Outputs['model_input'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "S3 path to model artifacts with integrated hyperparameters"
        }
      ]
    }
  },
  "level4": {
    "passed": false,
    "issues": [
      {
        "severity": "ERROR",
        "category": "missing_configuration",
        "message": "Configuration file not found for dummy_training",
        "details": {
          "searched_patterns": [
            "config_dummy_training_step.py",
            "FlexibleFileResolver patterns",
            "Fuzzy matching"
          ],
          "search_directory": "src/cursus/steps/configs"
        },
        "recommendation": "Create configuration file config_dummy_training_step.py"
      }
    ]
  },
  "overall_status": "FAILING",
  "metadata": {
    "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/dummy_training.py",
    "contract_mapping": "dummy_training_contract",
    "validation_timestamp": "2025-08-11T08:17:50.251380",
    "validator_version": "1.0.0"
  }
}