{
  "script_name": "model_calibration",
  "level1": {
    "passed": true,
    "issues": [],
    "script_analysis": {
      "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/model_calibration.py",
      "path_references": [
        "path='Model Calibration Script for SageMaker Processing.\\n\\nThis script calibrates model prediction scores to accurate probabilities,\\nwhich is essential for risk-based decision-making and threshold setting.\\nIt supports multiple calibration methods including GAM, Isotonic Regression,\\nand Platt Scaling, with options for monotonicity constraints.\\nIt supports both binary and multi-class classification scenarios.\\n' line_number=2 context='#!/usr/bin/env python\\n>>> \"\"\"Model Calibration Script for SageMaker Processing.\\n\\nThis script calibrates model prediction scores to accurate probabilities,' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/eval_data' line_number=40 context='\\n# Define standard SageMaker paths\\n>>> INPUT_DATA_PATH = \"/opt/ml/processing/input/eval_data\"\\nOUTPUT_CALIBRATION_PATH = \"/opt/ml/processing/output/calibration\"\\nOUTPUT_METRICS_PATH = \"/opt/ml/processing/output/metrics\"' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/calibration' line_number=41 context='# Define standard SageMaker paths\\nINPUT_DATA_PATH = \"/opt/ml/processing/input/eval_data\"\\n>>> OUTPUT_CALIBRATION_PATH = \"/opt/ml/processing/output/calibration\"\\nOUTPUT_METRICS_PATH = \"/opt/ml/processing/output/metrics\"\\nOUTPUT_CALIBRATED_DATA_PATH = \"/opt/ml/processing/output/calibrated_data\"' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/metrics' line_number=42 context='INPUT_DATA_PATH = \"/opt/ml/processing/input/eval_data\"\\nOUTPUT_CALIBRATION_PATH = \"/opt/ml/processing/output/calibration\"\\n>>> OUTPUT_METRICS_PATH = \"/opt/ml/processing/output/metrics\"\\nOUTPUT_CALIBRATED_DATA_PATH = \"/opt/ml/processing/output/calibrated_data\"\\n' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/calibrated_data' line_number=43 context='OUTPUT_CALIBRATION_PATH = \"/opt/ml/processing/output/calibration\"\\nOUTPUT_METRICS_PATH = \"/opt/ml/processing/output/metrics\"\\n>>> OUTPUT_CALIBRATED_DATA_PATH = \"/opt/ml/processing/output/calibrated_data\"\\n\\nclass CalibrationConfig:' is_hardcoded=True construction_method=None",
        "path='Configuration class for model calibration.' line_number=46 context='\\nclass CalibrationConfig:\\n>>>     \"\"\"Configuration class for model calibration.\"\"\"\\n    \\n    def __init__(' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/eval_data' line_number=50 context='    def __init__(\\n        self,\\n>>>         input_data_path: str = \"/opt/ml/processing/input/eval_data\",\\n        output_calibration_path: str = \"/opt/ml/processing/output/calibration\",\\n        output_metrics_path: str = \"/opt/ml/processing/output/metrics\",' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/calibration' line_number=51 context='        self,\\n        input_data_path: str = \"/opt/ml/processing/input/eval_data\",\\n>>>         output_calibration_path: str = \"/opt/ml/processing/output/calibration\",\\n        output_metrics_path: str = \"/opt/ml/processing/output/metrics\",\\n        output_calibrated_data_path: str = \"/opt/ml/processing/output/calibrated_data\",' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/metrics' line_number=52 context='        input_data_path: str = \"/opt/ml/processing/input/eval_data\",\\n        output_calibration_path: str = \"/opt/ml/processing/output/calibration\",\\n>>>         output_metrics_path: str = \"/opt/ml/processing/output/metrics\",\\n        output_calibrated_data_path: str = \"/opt/ml/processing/output/calibrated_data\",\\n        calibration_method: str = \"gam\",' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/calibrated_data' line_number=53 context='        output_calibration_path: str = \"/opt/ml/processing/output/calibration\",\\n        output_metrics_path: str = \"/opt/ml/processing/output/metrics\",\\n>>>         output_calibrated_data_path: str = \"/opt/ml/processing/output/calibrated_data\",\\n        calibration_method: str = \"gam\",\\n        label_field: str = \"label\",' is_hardcoded=True construction_method=None",
        "path='Initialize configuration with paths and parameters.' line_number=65 context='        multiclass_categories: Optional[List[str]] = None\\n    ):\\n>>>         \"\"\"Initialize configuration with paths and parameters.\"\"\"\\n        # I/O Paths\\n        self.input_data_path = input_data_path' is_hardcoded=True construction_method=None",
        "path='Create configuration from environment variables.' line_number=93 context='    @classmethod\\n    def from_env(cls):\\n>>>         \"\"\"Create configuration from environment variables.\"\"\"\\n        # Parse multiclass categories from environment\\n        multiclass_categories = None' is_hardcoded=True construction_method=None",
        "path='0.05' line_number=117 context='            monotonic_constraint=os.environ.get(\"MONOTONIC_CONSTRAINT\", \"True\").lower() == \"true\",\\n            gam_splines=int(os.environ.get(\"GAM_SPLINES\", \"10\")),\\n>>>             error_threshold=float(os.environ.get(\"ERROR_THRESHOLD\", \"0.05\")),\\n            num_classes=int(os.environ.get(\"NUM_CLASSES\", \"2\")),\\n            score_field_prefix=os.environ.get(\"SCORE_FIELD_PREFIX\", \"prob_class_\"),' is_hardcoded=True construction_method=None",
        "path=\"Create output directories if they don't exist.\" line_number=125 context='\\ndef create_directories(config=None):\\n>>>     \"\"\"Create output directories if they don\\'t exist.\"\"\"\\n    config = config or CalibrationConfig.from_env()\\n    os.makedirs(config.output_calibration_path, exist_ok=True)' is_hardcoded=True construction_method=None",
        "path='.csv' line_number=152 context='    \\n    for fname in sorted(os.listdir(data_dir)):\\n>>>         if fname.lower().endswith((\".csv\", \".parquet\", \".json\")):\\n            return os.path.join(data_dir, fname)\\n    ' is_hardcoded=True construction_method=None",
        "path='.json' line_number=152 context='    \\n    for fname in sorted(os.listdir(data_dir)):\\n>>>         if fname.lower().endswith((\".csv\", \".parquet\", \".json\")):\\n            return os.path.join(data_dir, fname)\\n    ' is_hardcoded=True construction_method=None",
        "path='.csv' line_number=177 context=\"    if data_file.endswith('.parquet'):\\n        df = pd.read_parquet(data_file)\\n>>>     elif data_file.endswith('.csv'):\\n        df = pd.read_csv(data_file)\\n    else:\" is_hardcoded=True construction_method=None",
        "path='Compute comprehensive calibration metrics including ECE, MCE, and reliability diagram.\\n    \\n    This function calculates:\\n    - Expected Calibration Error (ECE): weighted average of absolute calibration errors\\n    - Maximum Calibration Error (MCE): maximum calibration error across all bins\\n    - Reliability diagram data: points for plotting calibration curve\\n    - Bin statistics: detailed information about each probability bin\\n    - Brier score: quadratic scoring rule for probabilistic predictions\\n    - Preservation of discrimination: comparison of AUC before/after calibration\\n    \\n    Args:\\n        y_true: Ground truth binary labels (0/1)\\n        y_prob: Predicted probabilities\\n        n_bins: Number of bins for calibration curve\\n        \\n    Returns:\\n        Dict: Dictionary containing calibration metrics\\n    ' line_number=412 context='\\ndef compute_calibration_metrics(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10) -> Dict[str, Any]:\\n>>>     \"\"\"Compute comprehensive calibration metrics including ECE, MCE, and reliability diagram.\\n    \\n    This function calculates:' is_hardcoded=True construction_method=None",
        "path='reliability_diagram.png' line_number=621 context='    \\n    # Save figure\\n>>>     figure_path = os.path.join(config.output_metrics_path, \"reliability_diagram.png\")\\n    plt.savefig(figure_path)\\n    plt.close(fig)' is_hardcoded=False construction_method='os.path.join'",
        "path='reliability_diagram.png' line_number=621 context='    \\n    # Save figure\\n>>>     figure_path = os.path.join(config.output_metrics_path, \"reliability_diagram.png\")\\n    plt.savefig(figure_path)\\n    plt.close(fig)' is_hardcoded=True construction_method=None",
        "path='multiclass_reliability_diagram.png' line_number=707 context='    \\n    plt.tight_layout()\\n>>>     figure_path = os.path.join(config.output_metrics_path, \"multiclass_reliability_diagram.png\")\\n    plt.savefig(figure_path)\\n    plt.close(fig)' is_hardcoded=False construction_method='os.path.join'",
        "path='multiclass_reliability_diagram.png' line_number=707 context='    \\n    plt.tight_layout()\\n>>>     figure_path = os.path.join(config.output_metrics_path, \"multiclass_reliability_diagram.png\")\\n    plt.savefig(figure_path)\\n    plt.close(fig)' is_hardcoded=True construction_method=None",
        "path='Main entry point for the calibration script.' line_number=715 context='\\ndef main(config=None):\\n>>>     \"\"\"Main entry point for the calibration script.\"\"\"\\n    try:\\n        # Use provided config or create from environment' is_hardcoded=True construction_method=None",
        "path='calibration_metrics.json' line_number=785 context='            \\n            # Save metrics report\\n>>>             metrics_path = os.path.join(config.output_metrics_path, \"calibration_metrics.json\")\\n            with open(metrics_path, \"w\") as f:\\n                json.dump(metrics_report, f, indent=2)' is_hardcoded=False construction_method='os.path.join'",
        "path='calibration_metrics.json' line_number=785 context='            \\n            # Save metrics report\\n>>>             metrics_path = os.path.join(config.output_metrics_path, \"calibration_metrics.json\")\\n            with open(metrics_path, \"w\") as f:\\n                json.dump(metrics_report, f, indent=2)' is_hardcoded=True construction_method=None",
        "path='calibration_model.joblib' line_number=790 context='            \\n            # Save calibrator model\\n>>>             calibrator_path = os.path.join(config.output_calibration_path, \"calibration_model.joblib\")\\n            joblib.dump(calibrator, calibrator_path)\\n            ' is_hardcoded=False construction_method='os.path.join'",
        "path='calibrated_data.parquet' line_number=795 context='            # Add calibrated scores to dataframe and save\\n            df[\"calibrated_\" + config.score_field] = y_prob_calibrated\\n>>>             output_path = os.path.join(config.output_calibrated_data_path, \"calibrated_data.parquet\")\\n            df.to_parquet(output_path, index=False)\\n            ' is_hardcoded=False construction_method='os.path.join'",
        "path='calibration_summary.json' line_number=813 context='            }\\n            \\n>>>             summary_path = os.path.join(config.output_calibration_path, \"calibration_summary.json\")\\n            with open(summary_path, \"w\") as f:\\n                json.dump(summary, f, indent=2)' is_hardcoded=False construction_method='os.path.join'",
        "path='calibration_summary.json' line_number=813 context='            }\\n            \\n>>>             summary_path = os.path.join(config.output_calibration_path, \"calibration_summary.json\")\\n            with open(summary_path, \"w\") as f:\\n                json.dump(summary, f, indent=2)' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=823 context='                logger.warning(\"Calibration only marginally improved expected calibration error\")\\n                \\n>>>             logger.info(f\"Binary calibration complete. ECE reduced from {uncalibrated_metrics[\\'expected_calibration_error\\']:.4f} to {calibrated_metrics[\\'expected_calibration_error\\']:.4f}\")\\n            \\n        else:' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=823 context='                logger.warning(\"Calibration only marginally improved expected calibration error\")\\n                \\n>>>             logger.info(f\"Binary calibration complete. ECE reduced from {uncalibrated_metrics[\\'expected_calibration_error\\']:.4f} to {calibrated_metrics[\\'expected_calibration_error\\']:.4f}\")\\n            \\n        else:' is_hardcoded=True construction_method=None",
        "path='calibration_metrics.json' line_number=874 context='            \\n            # Save metrics report\\n>>>             metrics_path = os.path.join(config.output_metrics_path, \"calibration_metrics.json\")\\n            with open(metrics_path, \"w\") as f:\\n                json.dump(metrics_report, f, indent=2)' is_hardcoded=False construction_method='os.path.join'",
        "path='calibration_metrics.json' line_number=874 context='            \\n            # Save metrics report\\n>>>             metrics_path = os.path.join(config.output_metrics_path, \"calibration_metrics.json\")\\n            with open(metrics_path, \"w\") as f:\\n                json.dump(metrics_report, f, indent=2)' is_hardcoded=True construction_method=None",
        "path='calibration_models' line_number=879 context='            \\n            # Save calibrator models\\n>>>             calibrator_dir = os.path.join(config.output_calibration_path, \"calibration_models\")\\n            os.makedirs(calibrator_dir, exist_ok=True)\\n            ' is_hardcoded=False construction_method='os.path.join'",
        "path='calibrated_data.parquet' line_number=895 context='                df[f\"calibrated_{col_name}\"] = y_prob_calibrated[:, i]\\n            \\n>>>             output_path = os.path.join(config.output_calibrated_data_path, \"calibrated_data.parquet\")\\n            df.to_parquet(output_path, index=False)\\n            ' is_hardcoded=False construction_method='os.path.join'",
        "path='calibration_summary.json' line_number=916 context='            }\\n            \\n>>>             summary_path = os.path.join(config.output_calibration_path, \"calibration_summary.json\")\\n            with open(summary_path, \"w\") as f:\\n                json.dump(summary, f, indent=2)' is_hardcoded=False construction_method='os.path.join'",
        "path='calibration_summary.json' line_number=916 context='            }\\n            \\n>>>             summary_path = os.path.join(config.output_calibration_path, \"calibration_summary.json\")\\n            with open(summary_path, \"w\") as f:\\n                json.dump(summary, f, indent=2)' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=927 context='                \\n            logger.info(f\"Multi-class calibration complete. Macro ECE reduced from \" +\\n>>>                       f\"{uncalibrated_metrics[\\'macro_expected_calibration_error\\']:.4f} to \" +\\n                      f\"{calibrated_metrics[\\'macro_expected_calibration_error\\']:.4f}\")\\n        ' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=928 context='            logger.info(f\"Multi-class calibration complete. Macro ECE reduced from \" +\\n                      f\"{uncalibrated_metrics[\\'macro_expected_calibration_error\\']:.4f} to \" +\\n>>>                       f\"{calibrated_metrics[\\'macro_expected_calibration_error\\']:.4f}\")\\n        \\n        logger.info(f\"All outputs saved to: {config.output_calibration_path}, {config.output_metrics_path}, and {config.output_calibrated_data_path}\")' is_hardcoded=True construction_method=None"
      ],
      "env_var_accesses": [
        "variable_name='IS_BINARY' line_number=96 context='        # Parse multiclass categories from environment\\n        multiclass_categories = None\\n>>>         if os.environ.get(\"IS_BINARY\", \"True\").lower() != \"true\":\\n            multiclass_cats = os.environ.get(\"MULTICLASS_CATEGORIES\", None)\\n            if multiclass_cats:' access_method='os.environ.get' has_default=True default_value='True'",
        "variable_name='MULTICLASS_CATEGORIES' line_number=97 context='        multiclass_categories = None\\n        if os.environ.get(\"IS_BINARY\", \"True\").lower() != \"true\":\\n>>>             multiclass_cats = os.environ.get(\"MULTICLASS_CATEGORIES\", None)\\n            if multiclass_cats:\\n                try:' access_method='os.environ.get' has_default=True default_value=None",
        "variable_name='CALIBRATION_METHOD' line_number=111 context='            output_metrics_path=OUTPUT_METRICS_PATH,\\n            output_calibrated_data_path=OUTPUT_CALIBRATED_DATA_PATH,\\n>>>             calibration_method=os.environ.get(\"CALIBRATION_METHOD\", \"gam\"),\\n            label_field=os.environ.get(\"LABEL_FIELD\", \"label\"),\\n            score_field=os.environ.get(\"SCORE_FIELD\", \"prob_class_1\"),' access_method='os.environ.get' has_default=True default_value='gam'",
        "variable_name='LABEL_FIELD' line_number=112 context='            output_calibrated_data_path=OUTPUT_CALIBRATED_DATA_PATH,\\n            calibration_method=os.environ.get(\"CALIBRATION_METHOD\", \"gam\"),\\n>>>             label_field=os.environ.get(\"LABEL_FIELD\", \"label\"),\\n            score_field=os.environ.get(\"SCORE_FIELD\", \"prob_class_1\"),\\n            is_binary=os.environ.get(\"IS_BINARY\", \"True\").lower() == \"true\",' access_method='os.environ.get' has_default=True default_value='label'",
        "variable_name='SCORE_FIELD' line_number=113 context='            calibration_method=os.environ.get(\"CALIBRATION_METHOD\", \"gam\"),\\n            label_field=os.environ.get(\"LABEL_FIELD\", \"label\"),\\n>>>             score_field=os.environ.get(\"SCORE_FIELD\", \"prob_class_1\"),\\n            is_binary=os.environ.get(\"IS_BINARY\", \"True\").lower() == \"true\",\\n            monotonic_constraint=os.environ.get(\"MONOTONIC_CONSTRAINT\", \"True\").lower() == \"true\",' access_method='os.environ.get' has_default=True default_value='prob_class_1'",
        "variable_name='IS_BINARY' line_number=114 context='            label_field=os.environ.get(\"LABEL_FIELD\", \"label\"),\\n            score_field=os.environ.get(\"SCORE_FIELD\", \"prob_class_1\"),\\n>>>             is_binary=os.environ.get(\"IS_BINARY\", \"True\").lower() == \"true\",\\n            monotonic_constraint=os.environ.get(\"MONOTONIC_CONSTRAINT\", \"True\").lower() == \"true\",\\n            gam_splines=int(os.environ.get(\"GAM_SPLINES\", \"10\")),' access_method='os.environ.get' has_default=True default_value='True'",
        "variable_name='MONOTONIC_CONSTRAINT' line_number=115 context='            score_field=os.environ.get(\"SCORE_FIELD\", \"prob_class_1\"),\\n            is_binary=os.environ.get(\"IS_BINARY\", \"True\").lower() == \"true\",\\n>>>             monotonic_constraint=os.environ.get(\"MONOTONIC_CONSTRAINT\", \"True\").lower() == \"true\",\\n            gam_splines=int(os.environ.get(\"GAM_SPLINES\", \"10\")),\\n            error_threshold=float(os.environ.get(\"ERROR_THRESHOLD\", \"0.05\")),' access_method='os.environ.get' has_default=True default_value='True'",
        "variable_name='GAM_SPLINES' line_number=116 context='            is_binary=os.environ.get(\"IS_BINARY\", \"True\").lower() == \"true\",\\n            monotonic_constraint=os.environ.get(\"MONOTONIC_CONSTRAINT\", \"True\").lower() == \"true\",\\n>>>             gam_splines=int(os.environ.get(\"GAM_SPLINES\", \"10\")),\\n            error_threshold=float(os.environ.get(\"ERROR_THRESHOLD\", \"0.05\")),\\n            num_classes=int(os.environ.get(\"NUM_CLASSES\", \"2\")),' access_method='os.environ.get' has_default=True default_value='10'",
        "variable_name='ERROR_THRESHOLD' line_number=117 context='            monotonic_constraint=os.environ.get(\"MONOTONIC_CONSTRAINT\", \"True\").lower() == \"true\",\\n            gam_splines=int(os.environ.get(\"GAM_SPLINES\", \"10\")),\\n>>>             error_threshold=float(os.environ.get(\"ERROR_THRESHOLD\", \"0.05\")),\\n            num_classes=int(os.environ.get(\"NUM_CLASSES\", \"2\")),\\n            score_field_prefix=os.environ.get(\"SCORE_FIELD_PREFIX\", \"prob_class_\"),' access_method='os.environ.get' has_default=True default_value='0.05'",
        "variable_name='NUM_CLASSES' line_number=118 context='            gam_splines=int(os.environ.get(\"GAM_SPLINES\", \"10\")),\\n            error_threshold=float(os.environ.get(\"ERROR_THRESHOLD\", \"0.05\")),\\n>>>             num_classes=int(os.environ.get(\"NUM_CLASSES\", \"2\")),\\n            score_field_prefix=os.environ.get(\"SCORE_FIELD_PREFIX\", \"prob_class_\"),\\n            multiclass_categories=multiclass_categories' access_method='os.environ.get' has_default=True default_value='2'",
        "variable_name='SCORE_FIELD_PREFIX' line_number=119 context='            error_threshold=float(os.environ.get(\"ERROR_THRESHOLD\", \"0.05\")),\\n            num_classes=int(os.environ.get(\"NUM_CLASSES\", \"2\")),\\n>>>             score_field_prefix=os.environ.get(\"SCORE_FIELD_PREFIX\", \"prob_class_\"),\\n            multiclass_categories=multiclass_categories\\n        )' access_method='os.environ.get' has_default=True default_value='prob_class_'"
      ],
      "imports": [
        "module_name='os' import_alias=None line_number=11 is_from_import=False imported_items=[]",
        "module_name='sys' import_alias=None line_number=12 is_from_import=False imported_items=[]",
        "module_name='json' import_alias=None line_number=13 is_from_import=False imported_items=[]",
        "module_name='logging' import_alias=None line_number=14 is_from_import=False imported_items=[]",
        "module_name='traceback' import_alias=None line_number=15 is_from_import=False imported_items=[]",
        "module_name='typing' import_alias=None line_number=16 is_from_import=True imported_items=['Dict', 'List', 'Any', 'Optional', 'Tuple']",
        "module_name='numpy' import_alias='np' line_number=18 is_from_import=False imported_items=[]",
        "module_name='pandas' import_alias='pd' line_number=19 is_from_import=False imported_items=[]",
        "module_name='joblib' import_alias=None line_number=20 is_from_import=False imported_items=[]",
        "module_name='matplotlib.pyplot' import_alias='plt' line_number=21 is_from_import=False imported_items=[]",
        "module_name='sklearn.isotonic' import_alias=None line_number=22 is_from_import=True imported_items=['IsotonicRegression']",
        "module_name='sklearn.linear_model' import_alias=None line_number=23 is_from_import=True imported_items=['LogisticRegression']",
        "module_name='sklearn.calibration' import_alias=None line_number=24 is_from_import=True imported_items=['calibration_curve']",
        "module_name='sklearn.metrics' import_alias=None line_number=25 is_from_import=True imported_items=['brier_score_loss', 'roc_auc_score']",
        "module_name='pygam' import_alias=None line_number=29 is_from_import=True imported_items=['LogisticGAM', 's']"
      ],
      "argument_definitions": [],
      "file_operations": [
        "file_path='<file_object>' operation_type='write' line_number=787 context='            metrics_path = os.path.join(config.output_metrics_path, \"calibration_metrics.json\")\\n            with open(metrics_path, \"w\") as f:\\n>>>                 json.dump(metrics_report, f, indent=2)\\n            \\n            # Save calibrator model' mode=None method='json.dump'",
        "file_path='<file_object>' operation_type='write' line_number=815 context='            summary_path = os.path.join(config.output_calibration_path, \"calibration_summary.json\")\\n            with open(summary_path, \"w\") as f:\\n>>>                 json.dump(summary, f, indent=2)\\n            \\n            # Check if calibration improved by error threshold' mode=None method='json.dump'",
        "file_path='<file_object>' operation_type='write' line_number=876 context='            metrics_path = os.path.join(config.output_metrics_path, \"calibration_metrics.json\")\\n            with open(metrics_path, \"w\") as f:\\n>>>                 json.dump(metrics_report, f, indent=2)\\n            \\n            # Save calibrator models' mode=None method='json.dump'",
        "file_path='<file_object>' operation_type='write' line_number=918 context='            summary_path = os.path.join(config.output_calibration_path, \"calibration_summary.json\")\\n            with open(summary_path, \"w\") as f:\\n>>>                 json.dump(summary, f, indent=2)\\n            \\n            # Check if calibration improved by error threshold' mode=None method='json.dump'"
      ]
    },
    "contract": {
      "entry_point": "model_calibration.py",
      "inputs": {
        "evaluation_data": {
          "path": "/opt/ml/processing/input/eval_data"
        }
      },
      "outputs": {
        "calibration_output": {
          "path": "/opt/ml/processing/output/calibration"
        },
        "metrics_output": {
          "path": "/opt/ml/processing/output/metrics"
        },
        "calibrated_data": {
          "path": "/opt/ml/processing/output/calibrated_data"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [
          "CALIBRATION_METHOD",
          "LABEL_FIELD",
          "SCORE_FIELD",
          "IS_BINARY"
        ],
        "optional": {
          "MONOTONIC_CONSTRAINT": "True",
          "GAM_SPLINES": "10",
          "ERROR_THRESHOLD": "0.05",
          "NUM_CLASSES": "2",
          "SCORE_FIELD_PREFIX": "prob_class_",
          "MULTICLASS_CATEGORIES": "[0, 1]"
        }
      },
      "description": "Contract for model calibration processing step.\n    \n    The model calibration step takes a trained model's raw prediction scores and\n    calibrates them to better reflect true probabilities, which is essential for\n    risk-based decision-making, threshold setting, and confidence in model outputs.\n    Supports both binary and multi-class classification scenarios.\n    \n    Input Structure:\n    - /opt/ml/processing/input/eval_data: Evaluation dataset with ground truth labels and model predictions\n    \n    Output Structure:\n    - /opt/ml/processing/output/calibration: Calibration mapping and artifacts\n    - /opt/ml/processing/output/metrics: Calibration quality metrics\n    - /opt/ml/processing/output/calibrated_data: Dataset with calibrated probabilities\n    \n    Environment Variables:\n    - CALIBRATION_METHOD: Method to use for calibration (gam, isotonic, platt)\n    - LABEL_FIELD: Name of the label column\n    - SCORE_FIELD: Name of the prediction score column (for binary classification)\n    - IS_BINARY: Whether this is a binary classification task (true/false)\n    - MONOTONIC_CONSTRAINT: Whether to enforce monotonicity in GAM (optional)\n    - GAM_SPLINES: Number of splines for GAM (optional)\n    - ERROR_THRESHOLD: Acceptable calibration error threshold (optional)\n    - NUM_CLASSES: Number of classes for multi-class classification (optional, default=2)\n    - SCORE_FIELD_PREFIX: Prefix for probability columns in multi-class scenario (optional)\n    - MULTICLASS_CATEGORIES: JSON string of class names/values for multi-class (optional)\n    ",
      "framework_requirements": {
        "scikit-learn": ">=0.23.2,<1.0.0",
        "pandas": ">=1.2.0,<2.0.0",
        "numpy": ">=1.20.0",
        "pygam": ">=0.8.0",
        "matplotlib": ">=3.3.0"
      }
    }
  },
  "level2": {
    "passed": true,
    "issues": [],
    "contract": {
      "entry_point": "model_calibration.py",
      "inputs": {
        "evaluation_data": {
          "path": "/opt/ml/processing/input/eval_data"
        }
      },
      "outputs": {
        "calibration_output": {
          "path": "/opt/ml/processing/output/calibration"
        },
        "metrics_output": {
          "path": "/opt/ml/processing/output/metrics"
        },
        "calibrated_data": {
          "path": "/opt/ml/processing/output/calibrated_data"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [
          "CALIBRATION_METHOD",
          "LABEL_FIELD",
          "SCORE_FIELD",
          "IS_BINARY"
        ],
        "optional": {
          "MONOTONIC_CONSTRAINT": "True",
          "GAM_SPLINES": "10",
          "ERROR_THRESHOLD": "0.05",
          "NUM_CLASSES": "2",
          "SCORE_FIELD_PREFIX": "prob_class_",
          "MULTICLASS_CATEGORIES": "[0, 1]"
        }
      },
      "description": "Contract for model calibration processing step.\n    \n    The model calibration step takes a trained model's raw prediction scores and\n    calibrates them to better reflect true probabilities, which is essential for\n    risk-based decision-making, threshold setting, and confidence in model outputs.\n    Supports both binary and multi-class classification scenarios.\n    \n    Input Structure:\n    - /opt/ml/processing/input/eval_data: Evaluation dataset with ground truth labels and model predictions\n    \n    Output Structure:\n    - /opt/ml/processing/output/calibration: Calibration mapping and artifacts\n    - /opt/ml/processing/output/metrics: Calibration quality metrics\n    - /opt/ml/processing/output/calibrated_data: Dataset with calibrated probabilities\n    \n    Environment Variables:\n    - CALIBRATION_METHOD: Method to use for calibration (gam, isotonic, platt)\n    - LABEL_FIELD: Name of the label column\n    - SCORE_FIELD: Name of the prediction score column (for binary classification)\n    - IS_BINARY: Whether this is a binary classification task (true/false)\n    - MONOTONIC_CONSTRAINT: Whether to enforce monotonicity in GAM (optional)\n    - GAM_SPLINES: Number of splines for GAM (optional)\n    - ERROR_THRESHOLD: Acceptable calibration error threshold (optional)\n    - NUM_CLASSES: Number of classes for multi-class classification (optional, default=2)\n    - SCORE_FIELD_PREFIX: Prefix for probability columns in multi-class scenario (optional)\n    - MULTICLASS_CATEGORIES: JSON string of class names/values for multi-class (optional)\n    ",
      "framework_requirements": {
        "scikit-learn": ">=0.23.2,<1.0.0",
        "pandas": ">=1.2.0,<2.0.0",
        "numpy": ">=1.20.0",
        "pygam": ">=0.8.0",
        "matplotlib": ">=3.3.0"
      }
    },
    "specifications": {
      "model_calibration_spec": {
        "step_type": "ModelCalibration",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "evaluation_data",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "XGBoostTraining",
              "XGBoostModelEval",
              "TrainingEvaluation",
              "CrossValidation",
              "ModelEvaluation"
            ],
            "data_type": "S3Uri",
            "description": "Evaluation dataset with ground truth labels and model predictions"
          }
        ],
        "outputs": [
          {
            "logical_name": "calibration_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['calibration_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Calibration mapping and artifacts"
          },
          {
            "logical_name": "metrics_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Calibration quality metrics and visualizations"
          },
          {
            "logical_name": "calibrated_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['calibrated_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Dataset with calibrated probabilities"
          }
        ]
      }
    },
    "unified_specification": {
      "primary_spec": {
        "step_type": "ModelCalibration",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "evaluation_data",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "XGBoostTraining",
              "XGBoostModelEval",
              "TrainingEvaluation",
              "CrossValidation",
              "ModelEvaluation"
            ],
            "data_type": "S3Uri",
            "description": "Evaluation dataset with ground truth labels and model predictions"
          }
        ],
        "outputs": [
          {
            "logical_name": "calibration_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['calibration_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Calibration mapping and artifacts"
          },
          {
            "logical_name": "metrics_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Calibration quality metrics and visualizations"
          },
          {
            "logical_name": "calibrated_data",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['calibrated_data'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Dataset with calibrated probabilities"
          }
        ]
      },
      "variants": {
        "calibration": {
          "step_type": "ModelCalibration",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "evaluation_data",
              "dependency_type": "processing_output",
              "required": true,
              "compatible_sources": [
                "XGBoostTraining",
                "XGBoostModelEval",
                "TrainingEvaluation",
                "CrossValidation",
                "ModelEvaluation"
              ],
              "data_type": "S3Uri",
              "description": "Evaluation dataset with ground truth labels and model predictions"
            }
          ],
          "outputs": [
            {
              "logical_name": "calibration_output",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['calibration_output'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Calibration mapping and artifacts"
            },
            {
              "logical_name": "metrics_output",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Calibration quality metrics and visualizations"
            },
            {
              "logical_name": "calibrated_data",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['calibrated_data'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Dataset with calibrated probabilities"
            }
          ]
        }
      },
      "unified_dependencies": {
        "evaluation_data": {
          "logical_name": "evaluation_data",
          "dependency_type": "processing_output",
          "required": true,
          "compatible_sources": [
            "XGBoostTraining",
            "XGBoostModelEval",
            "TrainingEvaluation",
            "CrossValidation",
            "ModelEvaluation"
          ],
          "data_type": "S3Uri",
          "description": "Evaluation dataset with ground truth labels and model predictions"
        }
      },
      "unified_outputs": {
        "calibration_output": {
          "logical_name": "calibration_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['calibration_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Calibration mapping and artifacts"
        },
        "metrics_output": {
          "logical_name": "metrics_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Calibration quality metrics and visualizations"
        },
        "calibrated_data": {
          "logical_name": "calibrated_data",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['calibrated_data'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Dataset with calibrated probabilities"
        }
      },
      "dependency_sources": {
        "evaluation_data": [
          "calibration"
        ]
      },
      "output_sources": {
        "calibration_output": [
          "calibration"
        ],
        "metrics_output": [
          "calibration"
        ],
        "calibrated_data": [
          "calibration"
        ]
      },
      "variant_count": 1
    }
  },
  "level3": {
    "passed": true,
    "issues": [],
    "specification": {
      "step_type": "ModelCalibration",
      "node_type": "internal",
      "dependencies": [
        {
          "logical_name": "evaluation_data",
          "dependency_type": "processing_output",
          "required": true,
          "compatible_sources": [
            "XGBoostTraining",
            "XGBoostModelEval",
            "TrainingEvaluation",
            "CrossValidation",
            "ModelEvaluation"
          ],
          "data_type": "S3Uri",
          "description": "Evaluation dataset with ground truth labels and model predictions"
        }
      ],
      "outputs": [
        {
          "logical_name": "calibration_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['calibration_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Calibration mapping and artifacts"
        },
        {
          "logical_name": "metrics_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Calibration quality metrics and visualizations"
        },
        {
          "logical_name": "calibrated_data",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['calibrated_data'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Dataset with calibrated probabilities"
        }
      ]
    }
  },
  "level4": {
    "passed": false,
    "issues": [
      {
        "severity": "ERROR",
        "category": "missing_configuration",
        "message": "Configuration file not found for model_calibration",
        "details": {
          "builder_name": "model_calibration",
          "search_directory": "/Users/tianpeixie/github_workspace/cursus/test/steps/scripts/alignment_validation/src/cursus/steps/configs",
          "available_config_files": [],
          "available_base_names": [],
          "total_configs_found": 0,
          "resolver_strategies": [
            "Exact match",
            "Normalized matching (preprocess\u2194preprocessing, eval\u2194evaluation, xgb\u2194xgboost)",
            "Fuzzy matching (80% similarity threshold)"
          ]
        },
        "recommendation": "Check if config file exists with correct naming pattern, or create config_model_calibration_step.py"
      }
    ]
  },
  "overall_status": "FAILING",
  "metadata": {
    "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/model_calibration.py",
    "contract_mapping": "model_calibration_contract",
    "validation_timestamp": "2025-08-11T11:48:48.585267",
    "validator_version": "1.0.0"
  }
}