{
  "script_name": "mims_package",
  "level1": {
    "passed": true,
    "issues": [
      {
        "severity": "WARNING",
        "category": "path_usage",
        "message": "Contract declares path not used in script: /opt/ml/processing/input/calibration",
        "details": {
          "path": "/opt/ml/processing/input/calibration",
          "script": "mims_package"
        },
        "recommendation": "Either use path /opt/ml/processing/input/calibration in script or remove from contract"
      },
      {
        "severity": "INFO",
        "category": "file_operations",
        "message": "Contract declares input not read by script: /opt/ml/processing/input/calibration",
        "details": {
          "path": "/opt/ml/processing/input/calibration",
          "operation": "read",
          "script": "mims_package"
        },
        "recommendation": "Either read /opt/ml/processing/input/calibration in script or remove from contract inputs"
      }
    ],
    "script_analysis": {
      "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/mims_package.py",
      "path_references": [
        "path='/opt/ml/processing/input/model' line_number=18 context='\\n# Constants\\n>>> MODEL_PATH = Path(\"/opt/ml/processing/input/model\")\\nSCRIPT_PATH = Path(\"/opt/ml/processing/input/script\")\\nOUTPUT_PATH = Path(\"/opt/ml/processing/output\")' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/script' line_number=19 context='# Constants\\nMODEL_PATH = Path(\"/opt/ml/processing/input/model\")\\n>>> SCRIPT_PATH = Path(\"/opt/ml/processing/input/script\")\\nOUTPUT_PATH = Path(\"/opt/ml/processing/output\")\\nWORKING_DIRECTORY = Path(\"/tmp/mims_packaging_directory\")' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output' line_number=20 context='MODEL_PATH = Path(\"/opt/ml/processing/input/model\")\\nSCRIPT_PATH = Path(\"/opt/ml/processing/input/script\")\\n>>> OUTPUT_PATH = Path(\"/opt/ml/processing/output\")\\nWORKING_DIRECTORY = Path(\"/tmp/mims_packaging_directory\")\\nCODE_DIRECTORY = WORKING_DIRECTORY / \"code\"' is_hardcoded=True construction_method=None",
        "path='/tmp/mims_packaging_directory' line_number=21 context='SCRIPT_PATH = Path(\"/opt/ml/processing/input/script\")\\nOUTPUT_PATH = Path(\"/opt/ml/processing/output\")\\n>>> WORKING_DIRECTORY = Path(\"/tmp/mims_packaging_directory\")\\nCODE_DIRECTORY = WORKING_DIRECTORY / \"code\"\\n' is_hardcoded=True construction_method=None",
        "path='Ensure a directory exists, creating it if necessary.' line_number=26 context='\\ndef ensure_directory(directory: Path):\\n>>>     \"\"\"Ensure a directory exists, creating it if necessary.\"\"\"\\n    try:\\n        directory.mkdir(parents=True, exist_ok=True)' is_hardcoded=True construction_method=None",
        "path='Check if a file exists and log its details.' line_number=38 context='    \\ndef check_file_exists(path: Path, description: str) -> bool:\\n>>>     \"\"\"Check if a file exists and log its details.\"\"\"\\n    exists = path.exists() and path.is_file()\\n    try:' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=46 context='            logger.info(f\"{description}:\")\\n            logger.info(f\"  Path: {path}\")\\n>>>             logger.info(f\"  Size: {size_mb:.2f}MB\")\\n            logger.info(f\"  Permissions: {oct(stats.st_mode)[-3:]}\")\\n            logger.info(f\"  Last modified: {stats.st_mtime}\")' is_hardcoded=True construction_method=None",
        "path='List and log the contents of a directory.' line_number=58 context='\\ndef list_directory_contents(path: Path, description: str):\\n>>>     \"\"\"List and log the contents of a directory.\"\"\"\\n    logger.info(f\"\\\\n{\\'=\\'*20} Contents of {description} {\\'=\\'*20}\")\\n    logger.info(f\"Path: {path}\")' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=83 context='                    total_size += size_mb\\n                    file_count += 1\\n>>>                     logger.info(f\"{indent}\ud83d\udcc4 {item.name} ({size_mb:.2f}MB)\")\\n                elif item.is_dir():\\n                    dir_count += 1' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=93 context='        logger.info(f\"  Total files: {file_count}\")\\n        logger.info(f\"  Total directories: {dir_count}\")\\n>>>         logger.info(f\"  Total size: {total_size:.2f}MB\")\\n        \\n    except Exception as e:' is_hardcoded=True construction_method=None",
        "path='Copy a file and log the operation, ensuring destination directory exists.' line_number=100 context='    \\ndef copy_file_robust(src: Path, dst: Path):\\n>>>     \"\"\"Copy a file and log the operation, ensuring destination directory exists.\"\"\"\\n    logger.info(f\"\\\\nAttempting to copy file:\")\\n    logger.info(f\"  From: {src}\")' is_hardcoded=True construction_method=None",
        "path='Source file does not exist or is not a file. Skipping copy.' line_number=106 context='    \\n    if not check_file_exists(src, \"Source file for copy\"):\\n>>>         logger.warning(\"Source file does not exist or is not a file. Skipping copy.\")\\n        return False\\n    ' is_hardcoded=True construction_method=None",
        "path='Recursively copy scripts from source to destination.' line_number=124 context='\\ndef copy_scripts(src_dir: Path, dst_dir: Path):\\n>>>     \"\"\"Recursively copy scripts from source to destination.\"\"\"\\n    logger.info(f\"\\\\n{\\'=\\'*20} Copying Scripts {\\'=\\'*20}\")\\n    logger.info(f\"From: {src_dir}\")' is_hardcoded=True construction_method=None",
        "path='Source scripts directory does not exist or is not a directory. Skipping script copy.' line_number=132 context='\\n    if not src_dir.exists() or not src_dir.is_dir():\\n>>>         logger.warning(\"Source scripts directory does not exist or is not a directory. Skipping script copy.\")\\n        return\\n' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=150 context='    logger.info(f\"\\\\nScript copying summary:\")\\n    logger.info(f\"  Files copied: {files_copied}\")\\n>>>     logger.info(f\"  Total size: {total_size_mb:.2f}MB\")\\n    \\n    list_directory_contents(dst_dir, \"Destination scripts directory\")' is_hardcoded=True construction_method=None",
        "path='Extract a tar file to the specified path.' line_number=156 context='\\ndef extract_tarfile(tar_path: Path, extract_path: Path):\\n>>>     \"\"\"Extract a tar file to the specified path.\"\"\"\\n    logger.info(f\"\\\\n{\\'=\\'*20} Extracting Tar File {\\'=\\'*20}\")\\n    ' is_hardcoded=True construction_method=None",
        "path='Cannot extract. Tar file does not exist.' line_number=160 context='    \\n    if not check_file_exists(tar_path, \"Tar file to extract\"):\\n>>>         logger.error(\"Cannot extract. Tar file does not exist.\")\\n        return\\n    ' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=172 context='                size_mb = member.size / 1024 / 1024\\n                total_size += size_mb\\n>>>                 logger.info(f\"  {member.name} ({size_mb:.2f}MB)\")\\n            logger.info(f\"Total size in tar: {total_size:.2f}MB\")\\n            ' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=173 context='                total_size += size_mb\\n                logger.info(f\"  {member.name} ({size_mb:.2f}MB)\")\\n>>>             logger.info(f\"Total size in tar: {total_size:.2f}MB\")\\n            \\n            logger.info(f\"\\\\nExtracting to: {extract_path}\")' is_hardcoded=True construction_method=None",
        "path='Create a tar file from the contents of a directory.' line_number=186 context='\\ndef create_tarfile(output_tar_path: Path, source_dir: Path):\\n>>>     \"\"\"Create a tar file from the contents of a directory.\"\"\"\\n    logger.info(f\"\\\\n{\\'=\\'*20} Creating Tar File {\\'=\\'*20}\")\\n    logger.info(f\"Output tar: {output_tar_path}\")' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=204 context='                    total_size += size_mb\\n                    files_added += 1\\n>>>                     logger.info(f\"Adding to tar: {arcname} ({size_mb:.2f}MB)\")\\n                    tar.add(item, arcname=arcname)\\n        ' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=209 context='        logger.info(f\"\\\\nTar creation summary:\")\\n        logger.info(f\"  Files added: {files_added}\")\\n>>>         logger.info(f\"  Total uncompressed size: {total_size:.2f}MB\")\\n        \\n        if check_file_exists(output_tar_path, \"Created tar file\"):' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=213 context='        if check_file_exists(output_tar_path, \"Created tar file\"):\\n            compressed_size = output_tar_path.stat().st_size / 1024 / 1024\\n>>>             logger.info(f\"  Compressed tar size: {compressed_size:.2f}MB\")\\n            logger.info(f\"  Compression ratio: {compressed_size/total_size:.2%}\")\\n        ' is_hardcoded=True construction_method=None",
        "path='.2%' line_number=214 context='            compressed_size = output_tar_path.stat().st_size / 1024 / 1024\\n            logger.info(f\"  Compressed tar size: {compressed_size:.2f}MB\")\\n>>>             logger.info(f\"  Compression ratio: {compressed_size/total_size:.2%}\")\\n        \\n    except Exception as e:' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=224 context='    logger.info(f\"Python version: {sys.version}\")\\n    logger.info(f\"Working directory: {os.getcwd()}\")\\n>>>     logger.info(f\"Available disk space: {shutil.disk_usage(\\'/\\').free / (1024*1024*1024):.2f}GB\")\\n\\n    # Ensure working and output directories exist' is_hardcoded=True construction_method=None",
        "path='model.tar.gz' line_number=231 context='\\n    # Extract input model.tar.gz if it exists\\n>>>     input_model_tar = MODEL_PATH / \"model.tar.gz\"\\n    logger.info(\"\\\\nChecking for input model.tar.gz...\")\\n    ' is_hardcoded=True construction_method=None",
        "path='\\nChecking for input model.tar.gz...' line_number=232 context='    # Extract input model.tar.gz if it exists\\n    input_model_tar = MODEL_PATH / \"model.tar.gz\"\\n>>>     logger.info(\"\\\\nChecking for input model.tar.gz...\")\\n    \\n    if check_file_exists(input_model_tar, \"Input model.tar.gz\"):' is_hardcoded=True construction_method=None",
        "path='Input model.tar.gz' line_number=234 context='    logger.info(\"\\\\nChecking for input model.tar.gz...\")\\n    \\n>>>     if check_file_exists(input_model_tar, \"Input model.tar.gz\"):\\n        extract_tarfile(input_model_tar, WORKING_DIRECTORY)\\n    else:' is_hardcoded=True construction_method=None",
        "path='No model.tar.gz found. Copying all files from MODEL_PATH...' line_number=237 context='        extract_tarfile(input_model_tar, WORKING_DIRECTORY)\\n    else:\\n>>>         logger.info(\"No model.tar.gz found. Copying all files from MODEL_PATH...\")\\n        files_copied = 0\\n        total_size = 0' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=246 context='                    files_copied += 1\\n                    total_size += item.stat().st_size / 1024 / 1024\\n>>>         logger.info(f\"\\\\nCopied {files_copied} files, total size: {total_size:.2f}MB\")\\n\\n    # Copy inference scripts to WORKING_DIRECTORY/code' is_hardcoded=True construction_method=None",
        "path='model.tar.gz' line_number=252 context='\\n    # Create the output model.tar.gz\\n>>>     output_tar_file = OUTPUT_PATH / \"model.tar.gz\"\\n    create_tarfile(output_tar_file, WORKING_DIRECTORY)\\n' is_hardcoded=True construction_method=None"
      ],
      "env_var_accesses": [],
      "imports": [
        "module_name='shutil' import_alias=None line_number=1 is_from_import=False imported_items=[]",
        "module_name='tarfile' import_alias=None line_number=2 is_from_import=False imported_items=[]",
        "module_name='pathlib' import_alias=None line_number=3 is_from_import=True imported_items=['Path']",
        "module_name='logging' import_alias=None line_number=4 is_from_import=False imported_items=[]",
        "module_name='os' import_alias=None line_number=5 is_from_import=False imported_items=[]",
        "module_name='typing' import_alias=None line_number=6 is_from_import=True imported_items=['List', 'Dict', 'Optional']",
        "module_name='sys' import_alias=None line_number=7 is_from_import=False imported_items=[]"
      ],
      "argument_definitions": [],
      "file_operations": []
    },
    "contract": {
      "entry_point": "mims_package.py",
      "inputs": {
        "model_input": {
          "path": "/opt/ml/processing/input/model"
        },
        "inference_scripts_input": {
          "path": "/opt/ml/processing/input/script"
        },
        "calibration_model": {
          "path": "/opt/ml/processing/input/calibration"
        }
      },
      "outputs": {
        "packaged_model": {
          "path": "/opt/ml/processing/output"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "\n    MIMS packaging script that:\n    1. Extracts model artifacts from input model directory or model.tar.gz\n    2. Includes calibration model if available\n    3. Copies inference scripts to code directory\n    4. Creates a packaged model.tar.gz file for deployment\n    4. Provides detailed logging of the packaging process\n    \n    Input Structure:\n    - /opt/ml/processing/input/model: Model artifacts (files or model.tar.gz)\n    - /opt/ml/processing/input/script: Inference scripts to include\n    - /opt/ml/processing/input/calibration: Optional calibration model artifacts\n    \n    Output Structure:\n    - /opt/ml/processing/output/model.tar.gz: Packaged model ready for deployment\n    ",
      "framework_requirements": {
        "python": ">=3.7"
      }
    }
  },
  "level2": {
    "passed": true,
    "issues": [],
    "contract": {
      "entry_point": "mims_package.py",
      "inputs": {
        "model_input": {
          "path": "/opt/ml/processing/input/model"
        },
        "inference_scripts_input": {
          "path": "/opt/ml/processing/input/script"
        },
        "calibration_model": {
          "path": "/opt/ml/processing/input/calibration"
        }
      },
      "outputs": {
        "packaged_model": {
          "path": "/opt/ml/processing/output"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "\n    MIMS packaging script that:\n    1. Extracts model artifacts from input model directory or model.tar.gz\n    2. Includes calibration model if available\n    3. Copies inference scripts to code directory\n    4. Creates a packaged model.tar.gz file for deployment\n    4. Provides detailed logging of the packaging process\n    \n    Input Structure:\n    - /opt/ml/processing/input/model: Model artifacts (files or model.tar.gz)\n    - /opt/ml/processing/input/script: Inference scripts to include\n    - /opt/ml/processing/input/calibration: Optional calibration model artifacts\n    \n    Output Structure:\n    - /opt/ml/processing/output/model.tar.gz: Packaged model ready for deployment\n    ",
      "framework_requirements": {
        "python": ">=3.7"
      }
    },
    "specifications": {
      "packaging_spec": {
        "step_type": "Package",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "model_input",
            "dependency_type": "model_artifacts",
            "required": true,
            "compatible_sources": [
              "XGBoostTraining",
              "ModelStep",
              "TrainingStep"
            ],
            "data_type": "S3Uri",
            "description": "Trained model artifacts to be packaged"
          },
          {
            "logical_name": "inference_scripts_input",
            "dependency_type": "custom_property",
            "required": false,
            "compatible_sources": [
              "ProcessingStep",
              "ScriptStep"
            ],
            "data_type": "String",
            "description": "Inference scripts and code for model deployment (can be local directory path or S3 URI)"
          },
          {
            "logical_name": "calibration_model",
            "dependency_type": "processing_output",
            "required": false,
            "compatible_sources": [
              "ModelCalibration"
            ],
            "data_type": "S3Uri",
            "description": "Calibration model and artifacts for probability calibration (optional)"
          }
        ],
        "outputs": [
          {
            "logical_name": "packaged_model",
            "output_type": "model_artifacts",
            "property_path": "properties.ProcessingOutputConfig.Outputs['packaged_model'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Packaged model ready for deployment"
          }
        ]
      }
    },
    "unified_specification": {
      "primary_spec": {
        "step_type": "Package",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "model_input",
            "dependency_type": "model_artifacts",
            "required": true,
            "compatible_sources": [
              "XGBoostTraining",
              "ModelStep",
              "TrainingStep"
            ],
            "data_type": "S3Uri",
            "description": "Trained model artifacts to be packaged"
          },
          {
            "logical_name": "inference_scripts_input",
            "dependency_type": "custom_property",
            "required": false,
            "compatible_sources": [
              "ProcessingStep",
              "ScriptStep"
            ],
            "data_type": "String",
            "description": "Inference scripts and code for model deployment (can be local directory path or S3 URI)"
          },
          {
            "logical_name": "calibration_model",
            "dependency_type": "processing_output",
            "required": false,
            "compatible_sources": [
              "ModelCalibration"
            ],
            "data_type": "S3Uri",
            "description": "Calibration model and artifacts for probability calibration (optional)"
          }
        ],
        "outputs": [
          {
            "logical_name": "packaged_model",
            "output_type": "model_artifacts",
            "property_path": "properties.ProcessingOutputConfig.Outputs['packaged_model'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Packaged model ready for deployment"
          }
        ]
      },
      "variants": {
        "generic": {
          "step_type": "Package",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "model_input",
              "dependency_type": "model_artifacts",
              "required": true,
              "compatible_sources": [
                "XGBoostTraining",
                "ModelStep",
                "TrainingStep"
              ],
              "data_type": "S3Uri",
              "description": "Trained model artifacts to be packaged"
            },
            {
              "logical_name": "inference_scripts_input",
              "dependency_type": "custom_property",
              "required": false,
              "compatible_sources": [
                "ProcessingStep",
                "ScriptStep"
              ],
              "data_type": "String",
              "description": "Inference scripts and code for model deployment (can be local directory path or S3 URI)"
            },
            {
              "logical_name": "calibration_model",
              "dependency_type": "processing_output",
              "required": false,
              "compatible_sources": [
                "ModelCalibration"
              ],
              "data_type": "S3Uri",
              "description": "Calibration model and artifacts for probability calibration (optional)"
            }
          ],
          "outputs": [
            {
              "logical_name": "packaged_model",
              "output_type": "model_artifacts",
              "property_path": "properties.ProcessingOutputConfig.Outputs['packaged_model'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Packaged model ready for deployment"
            }
          ]
        }
      },
      "unified_dependencies": {
        "model_input": {
          "logical_name": "model_input",
          "dependency_type": "model_artifacts",
          "required": true,
          "compatible_sources": [
            "XGBoostTraining",
            "ModelStep",
            "TrainingStep"
          ],
          "data_type": "S3Uri",
          "description": "Trained model artifacts to be packaged"
        },
        "inference_scripts_input": {
          "logical_name": "inference_scripts_input",
          "dependency_type": "custom_property",
          "required": false,
          "compatible_sources": [
            "ProcessingStep",
            "ScriptStep"
          ],
          "data_type": "String",
          "description": "Inference scripts and code for model deployment (can be local directory path or S3 URI)"
        },
        "calibration_model": {
          "logical_name": "calibration_model",
          "dependency_type": "processing_output",
          "required": false,
          "compatible_sources": [
            "ModelCalibration"
          ],
          "data_type": "S3Uri",
          "description": "Calibration model and artifacts for probability calibration (optional)"
        }
      },
      "unified_outputs": {
        "packaged_model": {
          "logical_name": "packaged_model",
          "output_type": "model_artifacts",
          "property_path": "properties.ProcessingOutputConfig.Outputs['packaged_model'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Packaged model ready for deployment"
        }
      },
      "dependency_sources": {
        "model_input": [
          "generic"
        ],
        "inference_scripts_input": [
          "generic"
        ],
        "calibration_model": [
          "generic"
        ]
      },
      "output_sources": {
        "packaged_model": [
          "generic"
        ]
      },
      "variant_count": 1
    }
  },
  "level3": {
    "passed": false,
    "issues": [
      {
        "severity": "ERROR",
        "category": "dependency_resolution",
        "message": "Cannot resolve required dependency: model_input",
        "details": {
          "logical_name": "model_input",
          "specification": "mims_package",
          "compatible_sources": [
            "XGBoostTraining",
            "ModelStep",
            "TrainingStep"
          ],
          "dependency_type": "model_artifacts",
          "available_steps": [
            "DataLoading",
            "Preprocessing",
            "CurrencyConversion",
            "ModelEval",
            "XgboostModel",
            "Registration",
            "RiskTableMapping",
            "BatchTransform",
            "Dummy",
            "Model",
            "Payload",
            "Xgboost",
            "Pytorch",
            "Packaging",
            "PytorchModel"
          ]
        },
        "recommendation": "Ensure a step exists that produces output model_input"
      }
    ],
    "specification": {
      "step_type": "Package",
      "node_type": "internal",
      "dependencies": [
        {
          "logical_name": "model_input",
          "dependency_type": "model_artifacts",
          "required": true,
          "compatible_sources": [
            "XGBoostTraining",
            "ModelStep",
            "TrainingStep"
          ],
          "data_type": "S3Uri",
          "description": "Trained model artifacts to be packaged"
        },
        {
          "logical_name": "inference_scripts_input",
          "dependency_type": "custom_property",
          "required": false,
          "compatible_sources": [
            "ProcessingStep",
            "ScriptStep"
          ],
          "data_type": "String",
          "description": "Inference scripts and code for model deployment (can be local directory path or S3 URI)"
        },
        {
          "logical_name": "calibration_model",
          "dependency_type": "processing_output",
          "required": false,
          "compatible_sources": [
            "ModelCalibration"
          ],
          "data_type": "S3Uri",
          "description": "Calibration model and artifacts for probability calibration (optional)"
        }
      ],
      "outputs": [
        {
          "logical_name": "packaged_model",
          "output_type": "model_artifacts",
          "property_path": "properties.ProcessingOutputConfig.Outputs['packaged_model'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Packaged model ready for deployment"
        }
      ]
    }
  },
  "level4": {
    "passed": false,
    "issues": [
      {
        "severity": "ERROR",
        "category": "missing_configuration",
        "message": "Configuration file not found for mims_package",
        "details": {
          "searched_patterns": [
            "config_mims_package_step.py",
            "FlexibleFileResolver patterns",
            "Fuzzy matching"
          ],
          "search_directory": "src/cursus/steps/configs"
        },
        "recommendation": "Create configuration file config_mims_package_step.py"
      }
    ]
  },
  "overall_status": "FAILING",
  "metadata": {
    "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/mims_package.py",
    "contract_mapping": "mims_package_contract",
    "validation_timestamp": "2025-08-11T08:17:50.263841",
    "validator_version": "1.0.0"
  }
}